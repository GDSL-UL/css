[
  {
    "objectID": "longitudinal-2.html",
    "href": "longitudinal-2.html",
    "title": "9  Assessing Interventions",
    "section": "",
    "text": "9.1 Dependencies\n# Data\nlibrary(sf)\nlibrary(readr)\nlibrary(tidyverse)\n\n# Dates and Times\nlibrary(lubridate)\n\n# graphs\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(gridExtra)\nlibrary(viridis)\nlibrary(ggimage)\nlibrary(grid)\nlibrary(plotly)\n\n# Models\nlibrary(modelsummary)\nlibrary(broom)\nlibrary(gtools)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Assessing Interventions</span>"
    ]
  },
  {
    "objectID": "longitudinal-2.html#data",
    "href": "longitudinal-2.html#data",
    "title": "9  Assessing Interventions",
    "section": "9.2 Data",
    "text": "9.2 Data\nFirst let’s import the Greater London COVID-19 data:\n\n# import csv \ncovid_cases_london &lt;- read.csv(\"data/longitudinal-2/covid_cases_london.csv\", header = TRUE)\n# check out the variables\ncolnames(covid_cases_london)\n\n [1] \"areaType\"                       \"areaName\"                      \n [3] \"areaCode\"                       \"date\"                          \n [5] \"newCasesBySpecimenDate\"         \"cumCasesBySpecimenDate\"        \n [7] \"newFirstEpisodesBySpecimenDate\" \"cumFirstEpisodesBySpecimenDate\"\n [9] \"newReinfectionsBySpecimenDate\"  \"cumReinfectionsBySpecimenDate\" \n\n\nThen we do the same for the Lazio area data, which is the Region of the capital of Italy, Rome. We are choosing this region because it did not see sharp peaks in COVID-19 cases during the winter of 2020/2021.\n\n# import csv \ncovid_cases_lazio &lt;- read.csv(\"data/longitudinal-2/covid_cases_lazio.csv\", header = TRUE)\n# check out the variables\ncolnames(covid_cases_lazio)\n\n [1] \"data\"                    \"stato\"                  \n [3] \"codice_regione\"          \"denominazione_regione\"  \n [5] \"codice_provincia\"        \"denominazione_provincia\"\n [7] \"sigla_provincia\"         \"lat\"                    \n [9] \"long\"                    \"totale_casi\"            \n\n\nFirst we need to clean up the data somewhat and rename some variables in both dataframes to have 4 variables:\n\ndate: year-month-day\ngeo: geographical region\ncases: number of COVID-19 cases that day\narea : Lazio (Rome) or London\n\n\n# Rename the variables in the Lombardia data frame\ncovid_cases_lazio_ren &lt;- covid_cases_lazio %&gt;%\n  rename(date = data , geo = denominazione_provincia, totalcases = totale_casi) \n\n# Group the data by region and calculate total cases for each day (not cumulative cases)\ncovid_cases_lazio_daily &lt;- covid_cases_lazio_ren %&gt;%\n  group_by(geo) %&gt;%\n   mutate(cases = totalcases - lag(totalcases, default = 0)) %&gt;%\n  select(date, geo, cases) %&gt;%\n  mutate(area = \"Rome (Lazio)\") %&gt;%\n  filter(cases &gt;= 0)\n\n#df_milan &lt;- covid_cases_lombardia_new %&gt;%\n#  filter(geo == \"Milano\", new_cases &gt;= 0)\n\n# Rename the variables in the first data frame\ncovid_cases_london_ren &lt;- covid_cases_london %&gt;%\n  rename(date = date , geo = areaName, cases = newCasesBySpecimenDate) %&gt;%\n  select(date, geo, cases) %&gt;%\n  mutate(area = \"London\")\n\n# Correct date format\ncovid_cases_london_ren$date &lt;- as.Date(covid_cases_london_ren$date)\ncovid_cases_lazio_daily$date &lt;- as.Date(covid_cases_lazio_daily$date)\n\n# Append the renamed data frame to the second data frame\ncovid_combined &lt;- rbind(covid_cases_london_ren, covid_cases_lazio_daily)\n\n# Add a variable of log of cases\ncovid_combined &lt;- covid_combined %&gt;% \n          mutate(log_cases = log(cases))",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Assessing Interventions</span>"
    ]
  },
  {
    "objectID": "longitudinal-2.html#data-exploration",
    "href": "longitudinal-2.html#data-exploration",
    "title": "9  Assessing Interventions",
    "section": "9.3 Data Exploration",
    "text": "9.3 Data Exploration\nSimilarly to the previous chapter. Let’s start by eyeballing the data.\n\n# Visualizing Cases in London\ncovid_cases_1 &lt;- ggplot(data = covid_cases_london_ren, aes(x = date, y = cases, color=geo)) +\n  geom_line() + \n  scale_color_viridis(discrete = TRUE, option=\"magma\") +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\")\n  labs(\n    x = \"\",\n    y = \"\",\n    title = \"Evolution in Covid-19 cases\",\n    color = \"Region\"\n  ) +\n  scale_x_date(date_breaks = \"6 months\")\n\nNULL\n\n  covid_cases_1\n\n\n\n\n\n\n\n# Visualizing Cases in Lazio (Rome)\ncovid_cases_2 &lt;- ggplot(data = covid_cases_lazio_daily, aes(x = date, y = cases, color=geo)) +\n  geom_line() + \n  scale_color_viridis(discrete = TRUE, option=\"magma\") +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\")\n  labs(\n    x = \"\",\n    y = \"\",\n    title = \"Evolution in Covid-19 cases\",\n    color = \"Region\"\n  ) +\n  scale_x_date(date_breaks = \"6 months\")\n\nNULL\n\ncovid_cases_2\n\n\n\n\n\n\n\n\nTo identify whether there is a time period where a lockdown was implemented in one location but not the other, and how cases evolved, we can plot aggregates of both locations in one plot.\n\n# Aggregate the data by region for each day\ncovid_combined_agg &lt;- aggregate(cases ~ area + date, data = covid_combined, FUN = sum)\n\n# Visualizing aggregated\ncovid_cases_3 &lt;- ggplot(data = covid_combined_agg, aes(x = date, y = cases, color=area)) +\n  geom_line() + \n  scale_color_manual(values=c(\"darkblue\", \"darkred\")) + # set individual colors for the areas\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\"\n  ) +\n  labs(\n    x = \"\",\n    y = \"\",\n    title = \"Evolution in Covid-19 cases\",\n    color = \"Region\"\n  ) +\n  scale_x_date(date_breaks = \"6 months\")\n\ncovid_cases_3\n\n\n\n\n\n\n\n\nFrom an initial look at the data, the 2020/2021 winter period seems interesting as there is a high increase in London cases but not as much as a peak in Lazio cases. In fact, after a quick review of COVID-19 lockdowns, we found that:\n\nOn the 5th of November 2020, the UK Prime Minister announced a second national lockdown, coming into force in England\nOn 4 November 2020, Italian Prime Minister Conte announced a new lockdown as well, however this lockdown divided the country into three zones depending on the severity of the pandemic, corresponding to red, orange and yellow zones. The Lazio region, was a yellow zone for the duration of this second lockdown. In yellow zones, the only restrictions included compulsory closing for restaurant and bar activities at 6 PM, and online education for high schools only.\n\n\n# Usual chart\ncovid_cases_4 &lt;- ggplot(data = covid_combined_agg, aes(x = date, y = cases, color=area)) +\n  geom_line() + \n  scale_color_manual(values=c(\"darkblue\", \"darkred\")) + # set individual colors for the areas\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\"\n  ) +\n  labs(\n    x = \"\",\n    y = \"\",\n    title = \"Evolution in Covid-19 cases\",\n    color = \"Region\"\n  ) +\n  scale_x_date(limit=c(as.Date(\"2020-08-01\"), as.Date(\"2021-01-15\"))) +\n geom_vline(xintercept=as.numeric(as.Date(\"2020-11-05\")), linetype=\"dashed\") +\n  annotate(\"text\", x=as.Date(\"2020-11-06\"), y=25000, label=\"Lockdown\", \n           color=\"black\", fontface=\"bold\", angle=0, hjust=0, vjust=0)\n\ncovid_cases_4\n\n\n\n\n\n\n\n\nWe could make some assumptions and set this up as a quasi experiment. In social science, researchers are often using natural or quasi experimental setting as randomized experiments can rarely be conducted. This involves splitting the population at hand into a treatment and control group.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Assessing Interventions</span>"
    ]
  },
  {
    "objectID": "longitudinal-2.html#difference-in-difference",
    "href": "longitudinal-2.html#difference-in-difference",
    "title": "9  Assessing Interventions",
    "section": "9.4 Difference in Difference",
    "text": "9.4 Difference in Difference\nPlotting Means\nFor a diff-in-diff analysis using COVID data, possible shocks that would make this type of quasi-experiment possible could be the following:\n\nNational lockdown: The first national lockdown in the UK was announced on March 23, 2020. This sudden shock to the economy and society could be used as a treatment group for the diff-in-diff analysis, with the pre-lockdown period as the control group.\nRegional lockdowns: The UK also implemented regional lockdowns throughout the pandemic, with different regions experiencing restrictions at different times. These regional lockdowns could be used as treatment groups, with regions that did not experience lockdowns as the control group.\nSchool closures: In response to the pandemic, schools in the UK were closed from March 20, 2020, until June 1, 2020, and then again from January 5, 2021, until March 8, 2021. The impact of school closures on education outcomes could be studied using a diff-in-diff approach, with the period before school closures as the control group.\nTravel restrictions: The UK implemented various travel restrictions throughout the pandemic, including quarantine requirements for travelers from certain countries. The impact of these travel restrictions on the tourism industry or the spread of the virus could be studied using a diff-in-diff approach.\nVaccine rollout: The UK began its COVID-19 vaccination program in December 2020. The impact of the vaccine rollout on various health and economic outcomes could be studied using a diff-in-diff approach, with the period before the rollout as the control group.\n\nThese are just a few examples of shocks that could be used for a diff-in-diff analysis using COVID data. The choice of shock will depend on the research question and the data available.\nThe DiD approach includes a before-after comparison for a treatment and control group. In our example:\n\nA cross-sectional comparison (= compare a sample that was treated (London) to an non-treated control group (Rome))\nA before-after comparison (= compare treatment group with itself, before and after the treatment (5th of November))\n\nThe main assumption is that without the change in the natural environment the outcome variable would have remained constant!\nFirst, we create a dummy variable to indicate the time when the treatment started. In our case this will be the 5th of November 2020. We will also limit the time-span of our data.\n\n# keep data from 2020-09-01 to 2021-01-01\ncovid_combined_filtered &lt;- covid_combined %&gt;%\n  filter(date &gt;= \"2020-09-01\" & date &lt;= \"2021-01-01\")\n\n# create a dummy variable to indicate the time when the treatment started (5 Nov 2020)\ncovid_combined_filtered &lt;- covid_combined_filtered %&gt;%\n  mutate(after_5nov = ifelse(date &gt;= \"2020-11-05\", 1, 0)) #changed to 05 Nov\n\n\n# Create a frequency table of area and treatment\nfreq_table &lt;- table(covid_combined_filtered$area, covid_combined_filtered$after_5nov)\n\n# Print the frequency table\nprint(freq_table)\n\n              \n                  0    1\n  London       3150  540\n  Rome (Lazio) 1436  240\n\n\nWe then want to plot averages to see differences between treatment/control groups and before/after. But we can also calculate the mean and 95% confidence interval. We can also use group_by() and summarize() to figure out group means before sending the data to ggplot.\n\nplot_data &lt;- covid_combined_filtered %&gt;% \n  # Make these categories instead of 0/1 numbers so they look nicer in the plot\n  mutate(after_5nov = factor(after_5nov, labels = c(\"Before 5 November 2020\", \"After 5 November 2020\"))) %&gt;% \n  group_by(area, after_5nov) %&gt;% \n  summarize(mean_cases = mean(cases),\n            se_cases = sd(cases) / sqrt(n()),\n            upper = mean_cases + (1.96 * se_cases),\n            lower = mean_cases + (-1.96 * se_cases)) \n\nggplot(plot_data, aes(x = area, y = mean_cases)) +\n  geom_pointrange(aes(ymin = lower, ymax = upper), \n                  color = \"darkred\", size = 1) +\n  facet_wrap(vars(after_5nov))\n\n\n\n\n\n\n\n\nHere, we can start to see a diff-in-diff plot, where there is little to no difference in means with our control city (Rome-Lazio) and a substancial jump in means in our treatment city (London). It looks there were many more cases of COVID-19 after the 5th of march in London, indicating the lockdown did not have an effect, at least in this time-frame. Why could that be?\nWe can also plot a more standard diff-in-diff format:\n\nggplot(plot_data, aes(x = after_5nov, y = mean_cases, color = area)) +\n  geom_pointrange(aes(ymin = lower, ymax = upper), size = 1) + \n  geom_line(aes(group = area)) +\n  scale_color_manual(values = c(\"darkblue\", \"darkred\"))\n\n\n\n\n\n\n\n\nThis second plot shows us it is probable that our diff-n-diff set up will not work. A clean classic diff-n-diff would look more like the following. Please note the following plot is theoretical.\n\n# import csv \ncovid_perfect_example &lt;- read_csv(\"data/longitudinal-2/example_covid.csv\")\n\n# label pre/post labels\ncovid_perfect_example &lt;- covid_perfect_example %&gt;%\n  mutate(after_5nov = factor(after_5nov, labels = c(\"Before 5 November 2020\", \"After 5 November 2020\")))\n\n# plot    \nggplot(covid_perfect_example, aes(x = after_5nov, y = mean_cases, color = area)) +\n  geom_pointrange(aes(ymin = lower, ymax = upper), size = 1) + \n  geom_line(aes(group = area)) +\n  scale_color_manual(values = c(\"darkblue\", \"darkred\"))\n\n\n\n\n\n\n\n\nDifference in Difference by hand\nWe can find the exact difference by filling out the 2x2 before/after treatment/control table:\n\n\n\n\nBefore\nAfter\nDifference\n\n\n\n\nTreatment\nA\nB\nB - A\n\n\nControl\nC\nD\nD - C\n\n\nDifference\nC - A\nD - B\n(D − C) − (B − A)\n\n\n\nA combination of group_by() and summarize() makes this really easy. We can pull each of these numbers out of the table with some filter()s and pull():\n\nbefore_treatment &lt;- covid_perfect_example %&gt;% \n  filter(after_5nov == \"Before 5 November 2020\", area == \"London\") %&gt;% \n  pull(mean_cases)\n\nbefore_control &lt;- covid_perfect_example %&gt;% \n  filter(after_5nov == \"Before 5 November 2020\", area == \"Lazio\") %&gt;% \n  pull(mean_cases)\n\nafter_treatment &lt;- covid_perfect_example %&gt;% \n  filter(after_5nov == \"After 5 November 2020\", area == \"London\") %&gt;% \n  pull(mean_cases)\n\nafter_control &lt;- covid_perfect_example %&gt;% \n  filter(after_5nov == \"After 5 November 2020\", area == \"Lazio\") %&gt;% \n  pull(mean_cases)\n\ndiff_treatment_before_after &lt;- after_treatment - before_treatment\ndiff_treatment_before_after\n\n[1] 156.35\n\ndiff_control_before_after &lt;- after_control - before_control\ndiff_control_before_after\n\n[1] 24.35387\n\ndiff_diff &lt;- diff_treatment_before_after - diff_control_before_after\ndiff_diff\n\n[1] 131.9961\n\n\nThe diff-in-diff estimate is 131.99, which means that the lockdown here caused an increase in cases in the time-window we are analysing. Not it’s intended effect!\nWe can visualise this really well with a bit of extra code:\n\nggplot(covid_perfect_example, aes(x = after_5nov, y = mean_cases, color = area)) +\n  geom_point() +\n  #geom_pointrange(aes(ymin = lower, ymax = upper), size = 1) + \n  #geom_line(aes(group = area)) +\n  geom_line(aes(group = as.factor(area))) +\n  scale_color_manual(values = c(\"darkblue\", \"darkred\")) +\n  # If you use these lines you'll get some extra annotation lines and\n  # labels. The annotate() function lets you put stuff on a ggplot that's not\n  # part of a dataset. Normally with geom_line, geom_point, etc., you have to\n  # plot data that is in columns. With annotate() you can specify your own x and\n  # y values.\n  annotate(geom = \"segment\", x = \"Before 5 November 2020\", xend = \"After 5 November 2020\",\n           y = before_treatment, yend = after_treatment - diff_diff,\n           linetype = \"dashed\", color = \"grey50\") +\n  annotate(geom = \"segment\", x = \"After 5 November 2020\", xend = \"After 5 November 2020\",\n           y = after_treatment, yend = after_treatment - diff_diff,\n           linetype = \"dotted\", color = \"blue\") +\n  annotate(geom = \"label\", x = \"After 5 November 2020\", y = after_treatment - (diff_diff / 2), \n           label = \"Program effect\", size = 3)\n\n\n\n\n\n\n\n\nIt is important for all diff-in-diff analyses to give careful attention to possible violations of the common trends assumption, especially considering the COVID-19 situation where many of these violations are likely to occur. Furthermore, due to the unique dynamics of COVID-19 such as lags between exposure and recorded infections, nonlinearities from person-to-person transmission, and the possibility of policies having differential effects over time, it further complicates the potential risks to the diff-in-diff research design.\n(Goodman-Bacon and Marcus 2020) comment on the following problems which can be consulted in their paper:\n\nPackaged Policies\nReverse Causality\nVoluntary Precautions\nDifference Data collection\nAnticipation Spillovers\nVariation in Policy Timing\n\n(Goodman-Bacon and Marcus 2020) also give great recommendations on how to address these problems, but this is far beyond the objective of this chapter.\nDifference-in-Difference with regression\nCalculating all the pieces by hand like that is tedious, so we can use regression to do it instead! Remember that we need to include indicator variables for treatment/control and for before/after, as well as the interaction of the two.\nThis is the equation:\n\\(\\Delta Y_{gt} = \\beta_0 + \\beta_1 London_{g} + \\beta_2 Post5Nov_{t} + \\beta_3 London_{g} \\times Post5Nov_{t} + \\beta_4 Rome_{g} + \\epsilon_{gt}\\)\nThe output will show the diff-in-diff coefficient estimate, standard error, t-value, and p-value, which can be used to determine whether there was a significant effect of the second lockdown 4 on Covid cases in November 2020.\n\nmodel_small &lt;- lm(cases ~ area + after_5nov + area * after_5nov,\n                  data = covid_combined_filtered)\n\n# Tidy the model output\ndiffndiff1 &lt;- tidy(model_small) \n\n# Add significance stars using stars.pval from gtools\ndiffndiff1$stars &lt;- stars.pval(diffndiff1$p.value)\n\n# View the results\ndiffndiff1\n\n# A tibble: 4 × 6\n  term                        estimate std.error statistic   p.value stars\n  &lt;chr&gt;                          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;\n1 (Intercept)                     55.7      4.47      12.5 3.62e- 35 ***  \n2 areaRome (Lazio)               126.       7.99      15.8 8.81e- 55 ***  \n3 after_5nov                     301.      11.7       25.7 7.63e-138 ***  \n4 areaRome (Lazio):after_5nov   -269.      21.0      -12.8 5.37e- 37 ***  \n\n# Create a model summary table for the model\nsummary_table &lt;- modelsummary(list(\"Simple\" = model_small), estimate =c(\"{estimate}{stars}\"))\n\n# View the results\nsummary_table\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                Simple\n              \n        \n        \n        \n                \n                  (Intercept)\n                  55.694***\n                \n                \n                  \n                  (4.469)\n                \n                \n                  areaRome (Lazio)\n                  125.910***\n                \n                \n                  \n                  (7.986)\n                \n                \n                  after_5nov\n                  300.656***\n                \n                \n                  \n                  (11.681)\n                \n                \n                  areaRome (Lazio) × after_5nov\n                  -269.302***\n                \n                \n                  \n                  (21.032)\n                \n                \n                  Num.Obs.\n                  5366\n                \n                \n                  R2\n                  0.130\n                \n                \n                  R2 Adj.\n                  0.130\n                \n                \n                  AIC\n                  74524.8\n                \n                \n                  BIC\n                  74557.7\n                \n                \n                  Log.Lik.\n                  -37257.375\n                \n                \n                  F\n                  267.482\n                \n                \n                  RMSE\n                  250.71",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Assessing Interventions</span>"
    ]
  },
  {
    "objectID": "longitudinal-2.html#questions",
    "href": "longitudinal-2.html#questions",
    "title": "9  Assessing Interventions",
    "section": "9.5 Questions",
    "text": "9.5 Questions\nFor the assignment, you will continue to use Google Mobility data for the UK for 2021. For details on the timeline you can have a look here. You will need to do a bit of digging on when lockdowns or other COVID-19 related shock happened in 2021 to set up a diff-in-diff strategy. Have a look at Brodeur et al. (2021) to get some inspiration. They used Google Trends data to test whether COVID-19 and the associated lockdowns implemented in Europe and America led to changes in well-being.\nStart by loading both the csv\n\nmobility_gb &lt;- read.csv(\"data/longitudinal-1/2021_GB_Region_Mobility_Report.csv\", header = TRUE)\n\n\nVisualize the data with ggplot and identify what section of the data could be used to evaluate a COVID-19 intervention. Examples of these interventions could be a regional lockdown, school closures, travel restrictions or vaccine rollouts. Generate a clean ggplot which indicates which intervention you are going to examine.\nExplore differences in means through a frequency table and a graph of these averages. Chose whichever suits your purposes best.\nDefine and estimage a diff-in-diff regression. What do the results suggest? Was the intervention you chose effective? Discuss the reasons why it was or was not.\nDiscuss how the unique dynamics of COVID-19 and the possibility of policies having differential effects over time complicate the interpretation of your results.\n\nAnalyse and discuss what insights you obtain into people’s changes in behaviors during the pandemic in responde to an intervention.\n\n\n\n\nBrodeur, Abel, Andrew E Clark, Sarah Fleche, and Nattavudh Powdthavee. 2021. “COVID-19, Lockdowns and Well-Being: Evidence from Google Trends.” Journal of Public Economics 193: 104346.\n\n\nGoodman-Bacon, Andrew, and Jan Marcus. 2020. “Using Difference-in-Differences to Identify Causal Effects of COVID-19 Policies.”\n\n\nZhou, Muzhi, and Man-Yee Kan. 2021. “The Varying Impacts of COVID-19 and Its Related Measures in the UK: A Year in Review.” PLoS One 16 (9): e0257286.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Assessing Interventions</span>"
    ]
  },
  {
    "objectID": "machine-learning.html",
    "href": "machine-learning.html",
    "title": "10  Machine Learning",
    "section": "",
    "text": "10.1 Dependencies\n# Import the dplyr package for data manipulation\nsuppressMessages(library(dplyr))\n# Import the rpart package for decision tree modeling\nsuppressMessages(library(rpart))\n# Import the rpart.plot package for visualization of decision trees\nsuppressMessages(library(rpart.plot))\n# Import ggplot 2 to make plots\nsuppressMessages(library(ggplot2))\n# Import Metrics for performance metrics calculation\nsuppressMessages(library(Metrics))\n# Import caret for machine learning modeling and evaluation\nsuppressMessages(library(caret))\n# Import randomForest for the random forest algorithm\nsuppressMessages(library(randomForest))\n# Import ranger for the ranger implementation of random forest, which is optimised for performance\nsuppressMessages(library(ranger))\n# spatial data manipulation\nsuppressMessages(library(stars))",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "machine-learning.html#data",
    "href": "machine-learning.html#data",
    "title": "10  Machine Learning",
    "section": "10.2 Data",
    "text": "10.2 Data\nAs mentioned above, we will learn about decision trees and random forests through a practical example where the goal is to predict the median rent price in different UK locations based on the sociodemographic characteristics of those locations. Given that UK censuses take place separately in Northern Ireland, Scotland and England & Wales, we will focus only on England & Wales for simplicity.\nIn the code chunk below, we load a data set that has already been prepared for this notebook. It includes a variety of variables related to demographic characteristics of the population, aggregated at the Middle-Layer Super Output Area (MSOA) level. All the variables are derived from the 2021 census and the raw data can be downloaded from here.\n\n# Load the MSOA census data\ndf_MSOA &lt;- read.csv(\"./data/machine-learning/census2021-msoa.csv\")\n# Data cleaning, remove the X field\ndf_MSOA$X &lt;- NULL\n\nFor a description of the variables in the columns of df_MSOA, we can load a dictionary for these variables:\n\n# Load variable dictionary\ndf_dictionary &lt;- read.csv(\"./data/machine-learning/Dictionary.csv\")\nhead(df_dictionary)\n\n                                      Dictionary       X\n1                                                       \n2                                           Name     Key\n3                 Lives in household (% persons)    inHH\n4    Lives in communal establishment (% persons)    inCE\n5 Never married or civil partnership (% persons)    SING\n6    Married or in civil partnership (% persons) MARRIED\n\n\nThe variable we want to predict is the median rent price in each MSOA. To generate predictions, we first need to train a model based observed data. For this purpose, we use data from Zoopla, one of the UK’s leading websites for property listings. Median rent data corresponds to the last quarter of 2016 and is made available here for non-commercial use, through the Urban Big Data Centre. To facilitate usage in this chapter, we have done some pre-processing with the code available in cleaning-data.ipynb.\nIn the next few lines, we load the cleaned rent data from Zoopla, and we merge the relevant variable median_rent_per_month to the dataframe containing the sociodeomographic data from the census for each MSOA. Then, we discard three columns which are not relevant for the subsequent analysis date, geography and geography.code.\n\ndf_rent &lt;- read.csv(\"./data/machine-learning/zoopla_mean_rent_msoa.csv\")\n\n\ndf_MSOA &lt;- merge(df_MSOA, df_rent[, c(\"area_code\", \"median_rent_per_month\")], by.x = \"geography.code\", by.y = \"area_code\", all.x = TRUE) %&gt;% na.omit()\n\n\n# Data cleaning, the fields \"date\", \"geography\" and \"geography.code\" are not needed\ndf_MSOA &lt;- subset(df_MSOA, select = -c(date, geography, geography.code))",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "machine-learning.html#splitting-the-data",
    "href": "machine-learning.html#splitting-the-data",
    "title": "10  Machine Learning",
    "section": "10.3 Splitting the data",
    "text": "10.3 Splitting the data\nFor most supervised learning problems, the goal is to find an algorithm or model that not only fits well known data, but also accurately predicts unknown values of the median rent based on a set of inputs. In other words, we want the algorithm to be generalisable. In order to measure the generalisability of the optimal algorithm, we can split the data into a training set containing input and output data and a test set. The training set is used to teach the algorithm how to map inputs to outputs, and the test set is used to estimate the prediction error of the final algorithm, which quantifies the generalisability of the model. The test set should never be used during the training stage.\nAs a rule of thumb, the data should be split so that 70% of the samples are in the training set and 30% in the test set, although these percentages might vary slightly according to the size of the original data set. Furthermore, to ensure generalisability, the data split should be done so that the distribution of outputs in both the training and test set is approximately the same.\nThe function create_train_test below (borrowed from here) allows us to select samples from the data to create the training set (when the train parameter is set to TRUE) and the test set (when the train parameter is set to FALSE).\n\ncreate_train_test &lt;- function(data, size = 0.7, train = TRUE) {\n n_row = nrow(data)\n total_row = size * n_row\n train_sample &lt;- 1: total_row\n if (train == TRUE) {\n return (data[train_sample, ])\n } else {\n return (data[-train_sample, ])\n }\n}\n\n\ndf_train &lt;- create_train_test(df_MSOA, 0.7, train = TRUE)\ndf_test &lt;- create_train_test(df_MSOA, 0.7, train = FALSE)\n\nIf the data is naively split into training and test sets as we did above, the distribution of outputs in the training and test set will not be the same, as the following plot shows.\n\nd1 &lt;- density(df_test$median_rent_per_month)\nplot(d1, col='blue', xlab=\"Median rent per month\", main=\"Distribution of median rent per month\")\n\nd2 &lt;- density(df_train$median_rent_per_month)\nlines(d2, col='red')\n\nlegend(52000, 0.00006, c(\"test set\",\"training set\"), lwd=c(2,2), col=c(\"blue\",\"red\"))\n\n\n\n\n\n\n\n\nThis is due to the fact that the dataset that we loaded at the beginning of this workbook is sorted so that some entries corresponding to MSOAs that are geographically close to each other are in consecutive rows. Therefore, to ensure that the distribution of outputs in training and test sets is the same, the data needs to be randomly shuffled. The code below achieves this goal, as it can be seen in the kernel density plot with the new data split.\n\n# Shuffle the entries of the original dataset\nshuffle_index &lt;- sample(1:nrow(df_MSOA))\ndf_MSOA &lt;- df_MSOA[shuffle_index, ]\n\n\n#Perform the data split with the new shuffled dataset\ndf_train &lt;- create_train_test(df_MSOA, 0.7, train = TRUE)\ndf_test &lt;- create_train_test(df_MSOA, 0.7, train = FALSE)\n\n\n#Plot the kernel density for both training and test data sets\nd1 &lt;- density(df_test$median_rent_per_month)\nplot(d1, col='blue', xlab=\"Median rent per month\", main=\"Distribution of median rent per month\")\n\nd2 &lt;- density(df_train$median_rent_per_month)\nlines(d2, col='red')\n\nlegend(52000, 0.00006, c(\"test set\",\"training set\"), lwd=c(2,2), col=c(\"blue\",\"red\"))\n\n\n\n\n\n\n\n\nBefore proceeding any further we should note here that one of the advantages of decision trees and random forests is that they are not sensitive to the magnitude of the input variables, so standardisation is not needed before fitting these models. However, there are other ML algorithms such as k-means, where standardisation is a crucial step to ensure the success of learning process and it should always take place before training the algorithm. Similarly, random forests are not sensitive to correlations between independent variables, so there is no need to check for correlations before training the models.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "machine-learning.html#decision-trees",
    "href": "machine-learning.html#decision-trees",
    "title": "10  Machine Learning",
    "section": "10.4 Decision trees",
    "text": "10.4 Decision trees\n\n10.4.1 Fitting the training data\nDecision trees are an ML algorithm capable of performing both classification and regression tasks, although in this workbook we will focus only on regression. One of their most notable advantages is that they are highly interpretable, although their predictions are not always accurate. However, by aggregating decision trees through a method called bagging, the algorithm can become much more powerful.\nIn essence, a decision tree is like a flowchart that helps us make a decision based on a number of questions or conditions, which are represented by internal nodes. It starts with a root node and branches out into different paths, with each branch representing a decision. The final nodes represent the outcome and are known as leaf nodes. For example, imagine you are trying to decide what to wear to an event. You could create a decision tree with the root node being “Formal event?”, like in the diagram below. If the answer is “yes” you would proceed to the left and if the answer if “no”, you would proceed to the right. On the right, you would have another node for “Black tie?”, and again two “yes” and “no” branches emerging from it. On the left, you would have a node for “Everyday wear?” with its two branches. Each branch would eventually lead to a decision or action represented by the so-called leaf nodes, such as “wear suit”.\n\n\n\n\n\nThe decision tree to predict the median rent price in an MSOA based on its sociodemographic features will be a lot more complex than the one depicted above. The branches will lead to leaf nodes representing the predicted values of median rent price. The internal nodes will represent conditions for the demographic variables (e.g. is the percentage of people aged 65 or over in this MSOA greater than X %?). Not all the demographic variables will be equally relevant to predict the median rent at the MSOA level. To optimise the prediction process, conditions on the most relevant variables should be near the root of the tree so the most determining questions can be asked at the beginning of the decision-making process. The internal nodes that are further from the tree will help fine-tune the final predictions. But, how can we choose which are the most relevant variables? And, what are the right questions to ask in each internal node (e.g. if we are asking ‘is the percentage of people aged 65 or over in this MSOA greater than X %?’, what should be the value of X ?).\nLuckily, nowadays there are many off-the-shelf software packages available that can help us build decision trees with just a line of code. Here, we use the R package rpart, which is based on the Classification And Regression Tree (CART) algorithm proposed by Breiman (Breiman 1984). In particular, we can use the function rpart() to find the decision tree that best fits the training set. This function requires to use the formula method for expressing the model. In this case, the formula is median_rent_per_month ~., which means that we regard the variable median_rent_per_month as a function of all the other variables in the training data set. For more information on the formula method, you can check the R documentation. The function rpart() also requires to specify the method for fitting. Since we are performing a regression task (as opposed to classification), we need to set method to 'anova'.\n\nfit &lt;- rpart(as.numeric(median_rent_per_month) ~., data = df_train, method = 'anova')\n\nWe can visualise the fitted decision tree with the rpart.plot() function from the library with the same name. Interestingly, the condition for the root node is associated with the variable representing the percentage of people born in the British Americas and Caribbean, so if an MSOA has less than 2.5% people born in those territories, the model predicts that MSOA to have lower monthly median rent price. Can you think of why this might be the case?\n\nrpart.plot(fit)\n\n\n\n\n\n\n\n\nAs we can see, rpart() produces a decision tree with a number of leaf nodes and the conditions to reach each leaf are associated with only a few demographic variables. However, in the original training set, there were many more demographic variables that have not been included in the model. The reason for this is that, behind the scenes, rpart() is trying to find a balance between the complexity of the tree (i.e. its depth) and the generalisability of the tree to predict new unseen data. If it is too deep, the tree runs the risk of overfitting the training data and failing to predict the median monthly rent price for other MSOAs that are not included in the training set.\nTo illustrate the point of selecting a tree with the obtained number of leaves leaves, we can manually control the level of complexity allowed when fitting of a decision tree model. The lower we set the value of the parameter cp, the more complex the resulting tree will be, so cp can be regarded as penalty for the level of complexity or a cost complexity parameter. Below, we fit a decision tree with no penalty for generating a complex tree model, i.e. cp=0, and then we use the function plotcp() to plot the prediction error (PRESS statistic) that would be achieved with decision trees of different levels of complexity.\n\n# set seed for reproducibility\nset.seed(123)\n# train model\nfit2 &lt;- rpart(as.numeric(median_rent_per_month) ~., data = df_train, method = 'anova', control = list(cp = 0))\n# plot error vs tree complexity \nplotcp(fit2)\n\n\n\n\n\n\n\n\nAs we can see from the plot above, with more than a certain number of leaves, little reduction in the prediction error is achieved as the model becomes more and more complex. In other words, we start seeing diminishing returns in error reduction as the tree grows deeper. Hence rpart() is doing some behind-the-scenes tuning by pruning the tree so it only has the obtained number of leaf nodes.\nThere are other model parameters that can be tuned in order to improve the model performance via the control argument of the rpart() function, just like we did above for cp. While we do not experiment with these additional parameters in the workbook, we provide brief descriptions below so you can explore them on your own time:\n\nminsplit controls the minimum number of data points required in each leaf node. The default is 20. Setting this lower will result in more leaves with very few data points belonging to the corresponding branch.\nmaxdepth controls the maximum number of internal nodes between the root node and the terminal nodes. By default, it is set to 30. Setting it higher allows to create deeper trees.\n\n\n\n10.4.2 Measuring the performance of regression models\nTo measure the performance of the regression tree that we fitted above, we can use the test set. We firstly use the predict() function from the rpart library to compute some predictions on the MSOA median rent per month for the test set data.\n\npredict_unseen &lt;-predict(fit, df_test)\n\nThen, we compare the predictions with the actual values and measure the discrepancy with a regression error metric.\nNote that, to measure the performance of classification trees, the procedure would be slightly different and it would involve the computation of a confusion matrix and the accuracy metric.\nDifferent error metrics exist to measure the performance of regression models such as the Mean Squared Error (MSE), the Mean Absolute Error (MAE) or the Root Mean Squared Error (RMSE). The MSE is more sensitive to outliers than the MAE, however, the units of MSE are squared units. The RMSE solves the problem of the squared units associated with MSE by taking its squared root. The library Metrics provides the in-built function rmse() which makes the computation of RMSE straightforward:\n\nrmse(predict_unseen, df_test$median_rent_per_month)\n\n[1] 286.603\n\n\nThis value does not have much meaning in isolation. A good or bad RMSE is always relative to the specific data set. For this reason, we need to establish a baseline RMSE that we can compare it with. Here we establish this baseline as the RMSE that would be obtained from a naive tree that merely predicts the median monthly rent across all the data entries in the training set. If the fitted model achieves an RSME lower than the naive model, we say that the fitted model “has skill”. The following line of code confirms that our fitted model is better than the naive model.\n\nrmse(predict_unseen, mean(df_train$median_rent_per_month))\n\n[1] 386.3027\n\n\n\n\n10.4.3 Bagging\nEven though single decision trees have many advantages such as being very simple and interpretable, their predictions are not always accurate due to their high variance. This results in unstable predictions that may be dependent on the chosen training data.\nA method called bagging can help solve this issue by combining and averaging the predictions of multiple decision tree models. The method can actually be applied to any regression or classification model, however, it is most effective when applied to models that have high variance. Bagging works by following three steps:\n\nCreate m bootstrap samples from the training data (i.e. m random samples with replacement).\nFor each bootstrap sample, train an unpruned single tree (i.e. with cp=0).\nTo create a prediction for a new data point, input the data in the single trees fitted with each bootstrap sample. The prediction will be the average of all the individual predictions output by each tree.\n\nEach bootstrap sample typically contains about two-thirds of the training data, leaving one-third out. This left-out third is known as the out-of-bag (OOB) sample and it provides a natural opportunity to cross-validate the predictive performance of the model. By cross-validating the model, we can estimate how well our model will perform on new data without necessarily having to use the test set to test it. In other words, we can use the whole of the original data set for training and still quantify the performance of the model. However, for simplicity, we will train the model on the training set only here.\nBagging can be easily done with a library called caret. Here we fit a 10-fold cross-validated model, meaning that the bagging is applied so that there are 10 different OOB samples.\n\n# specify 10-fold cross validation\nctrl &lt;- trainControl(method = \"cv\",  number = 10) \n# set seed for reproducibility\nset.seed(123)\n# train the cross-validated bagged model\nbagged &lt;- caret::train(as.numeric(median_rent_per_month) ~ ., data = df_train, method = \"treebag\", trControl = ctrl, importance = TRUE)\n\nprint(bagged)\n\nBagged CART \n\n1754 samples\n  48 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 1578, 1578, 1578, 1580, 1578, 1579, ... \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  256.4799  0.7223646  174.7941\n\n\nWe see that the cross-validated value of RMSE with bagging is lower than that associated with the single decision tree that we trained with rcart. This indicates that the predictive performance is estimated to be better. We can compare the cross-validated value of the RMSE with the RMSE from the test set. These two quantities should be close:\n\npredict_bagged_unseen &lt;- predict(bagged, df_test)\nrmse(predict_bagged_unseen, df_test$median_rent_per_month)\n\n[1] 252.3735\n\n\nThe library caret has an additional function varImp() that helps us understand the variable importance across the bagged trees, i.e. the variables that are most relevant to determine the predictions of MSOA median rent per month. You are encouraged to check the caret documentation to learn more about how the variable importance is determined. We can plot a rank of variable importance by running the code below.\n\nplot(varImp(bagged), 20)\n\n\n\n\n\n\n\n\nAs noted before, given the variables included in our original data set, the percentage of people born in the British Americas and Caribbean is, almost invariably, the best predictor of median monthly rent, although due to the randomness introduced in bagging, this could sometimes change.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "machine-learning.html#random-forests",
    "href": "machine-learning.html#random-forests",
    "title": "10  Machine Learning",
    "section": "10.5 Random forests",
    "text": "10.5 Random forests\nWhile bagging considerably improves the performance of decision trees, the resulting models are still subject to some issues. Mainly, the multiple trees that are fitted through the bagging process are not completely independent of each other since all the original variables are considered at every split in every tree. As a consequence, trees in different bootstrap samples have similar structure (with almost always the same variables near the root) and the variance in the predictions cannot be reduced optimally. This issue is known as tree correlation.\nRandom forests optimally reduce the variance of the predicted values by minimising the tree correlation. This is achieved in two steps:\n\nLike in bagging, different trees are fitted from bootstrap samples.\nHowever, when an internal node is to be created in a given tree, the search for the optimal variable in that node is limited to only a random subset of the explanatory variables. By default, the number of variables in these subsets is one-third of the total number of variables, although this proportion is considered a tuning parameter for the model.\n\n\n10.5.1 Basic implementation\nSeveral R implementations for random forest fitting exist, however, the most well known is provided by the randomForest library. By default it performs 500 trees (i.e. 500 bootstrap samples) and randomly selects one-third of the explanatory variables for each split, although these parameters can be manually tuned. The random forest model can be trained by executing just a line of code:\n\n# set seed for reproducibility\nset.seed(123)\n# train model\nfit_rf &lt;- randomForest(formula = as.numeric(median_rent_per_month) ~., data = df_train)\n# print summary of fit\nfit_rf\n\n\nCall:\n randomForest(formula = as.numeric(median_rent_per_month) ~ .,      data = df_train) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 16\n\n          Mean of squared residuals: 51280.57\n                    % Var explained: 77.99\n\n\nAs we can see from above, the mean of squared residuals, which is the same as the MSE, is around 48,561 (you might get a slightly different number due to randomness in the algorithm) for 500 trees and therefore, RMSE = 220. This metric is computed by averaging residuals from the OOB samples. To illustrate how the MSE varies as more bootstrap samples are added to the model, we can plot the fitted model:\n\nplot(fit_rf, main = \"Errors vs no. of trees\")\n\n\n\n\n\n\n\n\nWe see that the MSE becomes stable with approximately 100-200 trees, but it continues to decrease slowly. To find the number of trees that lead to the minimum error, we can run the following line:\n\nwhich.min(fit_rf$mse)\n\n[1] 86\n\n\nBy computing the RMSE, we can compare the performance of this model with performance of the models in the previous sections:\n\nsqrt(fit_rf$mse[which.min(fit_rf$mse)])\n\n[1] 226.145\n\n\nThis is a lower value than what we obtained with just a single tree and even after applying bagging! Remember, this RMSE is based on the OOB samples, but we could also obtain it from the test set. randomForest() allows us to easily compare the RMSE obtained from OOB data and from the test set.\n\n# format test set for comparison of errors with randomForest\nx_test &lt;- df_test[setdiff(names(df_test), \"median_rent_per_month\")]\ny_test &lt;- df_test$median_rent_per_month\n\n# set seed for reproducibility\nset.seed(123)\n# include test data in training\nrf_oob_compare &lt;- randomForest(formula = as.numeric(median_rent_per_month) ~ ., data  = df_train, xtest = x_test, ytest = y_test)\n\n# extract OOB & test errors\noob_rmse &lt;- sqrt(rf_oob_compare$mse)\ntest_rmse &lt;- sqrt(rf_oob_compare$test$mse)\n\n# plot error vs no. of trees\nx=1:rf_oob_compare$ntree\nplot(x, oob_rmse, type=\"l\", col='blue', lwd=2, xlab = \"No. of trees\", ylab = \"RMSE\", main = \"Comparison of RMSE\")\nlines(x, test_rmse, type=\"l\", col='red', lwd=2)\nlegend(350, 4500, c(\"OOB\",\"test\"), lwd=c(2,2), col=c(\"blue\",\"red\"))\n\n\n\n\n\n\n\n\n\n\n10.5.2 Tuning\nYou may have noticed that the number of trees is not the only parameter we can tune in a random forest model. Below we list the model parameters, a.k.a. hyperparameters that can be tuned to improve the performance of the random tree models:\n\nnum.trees is the number of trees. It should be large enough to make sure the MSE (or the RMSE) stabilises, but not too large that it creates unnecessary work.\nmtry is the number of variables that are randomly sampled at each split. The default is one-third of the number of variables in the original data set. If mtry was equal to the total number of variables, the random forest model would be equivalent to bagging. Similarly, if mtry was equal to 1, it would mean that only one variable is chosen, but then the results can become too biased. To find the optimal value of mtry, it is common to attempt 5 values evenly spread between 2 and the total number of variables in the original data set.\nsample.fraction controls the number of data points in each bootstrap sample, i.e. the number of samples chosen to create each tree. By default, it is 63.25% (about two-thirds) of the training set since on average, this guarantees unique data points in a sample. If the sample size is smaller, it could reduce the training time but it could also introduce some bias in the model. If the sample size is larger, it could lead to overfitting. When tuning the model, this parameter is frequently kept between 60 and 80% of the total size of the training set.\nmin.node.size is the minimal node size to split at. Default is 5 for regression.\nmax.depth is the maximum depth of the trees.\n\nIn order to find the combination of hyperparameters that leads to the best performing model, we need to try them all and select the one with the lowest MSE or RMSE. This is usually a computationally heavy task, so as the models and the training data become larger, the process of tuning can become very slow. The library ranger provides a C++ implementation of the random forest algorithm and allows to perform hyperparameter search faster than randomForest.\nAs mentioned, to find the best performing model, we need to find the right combination of hyperparameters. So the first step in the tuning process is to generate a “grid” of possible combinations of hyperparameters. If we only wanted to tune ntree (as we did in the previous subsection when we found that the number of trees leading to the lowest value of MSE is 499), the grid would be simply a list of possible ntree values. To illustrate more complex tuning, here we generate a grid that considers mtry, sample.fraction and min.node.size. The grid is created as follows:\n\n# Considering that there are 48 explanatory variables in the original dataset, we will try values of mtry between 10 and 30. The sample size will go from 60% and 80% of the total size of the training set. We will try minimal node size splits between 5 and 20.\nhyper_grid &lt;- expand.grid(mtry = seq(10, 20, by=2), \n                          sample.fraction = c(0.60, 0.65, 0.70, 0.75, 0.80),\n                          min.node.size = seq(3, 9, by=2))\n\n# total number of hyperparameter combinations\nnrow(hyper_grid)\n\n[1] 120\n\n\nNext, we can loop through the grid and generate, for each hyperparameter combination, random forest models based on 500 trees. For each random forest model, we will add the OOB RMSE error to the grid so we can find what hyperparameter combination minimises this error. Note that we set the value of seed for code reproducibility purposes.\n\nfor(i in 1:nrow(hyper_grid)) {\n  \n  # train model\n  fit_rf_tuning &lt;- ranger(formula = as.numeric(median_rent_per_month) ~ ., \n    data = df_train, \n    num.trees = 500,\n    mtry = hyper_grid$mtry[i],\n    sample.fraction = hyper_grid$sample.fraction[i],\n    min.node.size = hyper_grid$min.node.size[i],\n    seed = 123)\n  \n  # add OOB error to grid\n  hyper_grid$OOB_RMSE[i] &lt;- sqrt(fit_rf_tuning$prediction.error)\n}\n\nFrom the fitted models, the one that produces the minimum OOB RMSE and hence, the best-performing one, is given by the combination of parameters printed below:\n\nhyper_grid[which.min(hyper_grid$OOB_RMSE),]\n\n   mtry sample.fraction min.node.size OOB_RMSE\n28   16             0.8             3 225.3778\n\n\nThe OOB RMSE is even lower than the error we obtained with the default model for random forest with no tuning.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "machine-learning.html#questions",
    "href": "machine-learning.html#questions",
    "title": "10  Machine Learning",
    "section": "10.6 Questions",
    "text": "10.6 Questions\nFor this set of questions, you will use a data set very similar to the one used in the examples above. However, instead of focusing on predicting the median rent per month, you will focus on predicting the relative population change in each MSOA between the year 2000 and 2021. We will use population data provided by WorldPop, like in Chapter 4. For the current application, the WorldPop data at 1km resolution has to be aggregated at the MSOA level, both for the year 2000 and the year 2020. We do this below.\nWe start by loading raster data for the year 2000 and 2020:\n\n# read_files\nraster2000 &lt;- read_stars(\"./data/machine-learning/gbr_ppp_2000_1km_Aggregated.tif\")\nraster2020 &lt;- read_stars(\"./data/machine-learning/gbr_ppp_2020_1km_Aggregated.tif\")\n\nWe then load the shapes of the MSOAs. These must be downloaded from UK’s GeoPortal here. Make sure you download the 2021 version and store it in the .\\data\\machine-learning\\ folder as a file with the .gpkg extension. We have not included the file in the GitHub repo due to its large size. You can load it with st_read and ensure it is in a projection system of choice:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Information Science and Statistics. New York: Springer.\n\n\nBoehmke, Brad. 2019. “Hands-on Machine Learning with r. Chapter 9: Decision Trees.” https://bradleyboehmke.github.io/HOML/DT.html.\n\n\nBreiman, L. 1984. Classification and Regression Trees (1st Ed.). Routledge.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "data-sets.html",
    "href": "data-sets.html",
    "title": "11  Data sets",
    "section": "",
    "text": "11.1 Greater Machester land use data",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data sets</span>"
    ]
  },
  {
    "objectID": "data-sets.html#greater-machester-land-use-data",
    "href": "data-sets.html#greater-machester-land-use-data",
    "title": "11  Data sets",
    "section": "",
    "text": "Availability\nThe dataset is stored on a gpkg file that can be found, within the structure of this project, under:\n\nst_LSOA &lt;- st_read(\"./data/geodemographics/manchester_land_cover_2011.gpkg\")\n\nReading layer `manchester_land_cover_2011' from data source \n  `/Users/carmen/Documents/github/r4ps/data/geodemographics/manchester_land_cover_2011.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1673 features and 44 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 351662.3 ymin: 381166 xmax: 406087.2 ymax: 421037.7\nProjected CRS: OSGB36 / British National Grid\n\n\n\n\nVariables\nThe variables included in this dataset follow the land use classification of the CORINE Land Cover dataset.\n\n\nSource & Pre-processing\nThe data was sourced from What do ‘left behind’ areas look like over time? and cleaned on Python.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data sets</span>"
    ]
  },
  {
    "objectID": "data-sets.html#british-administrative-boundaries-lsoas-msoas-and-las",
    "href": "data-sets.html#british-administrative-boundaries-lsoas-msoas-and-las",
    "title": "11  Data sets",
    "section": "11.2 British administrative boundaries (LSOAs, MSOAs and LAs)",
    "text": "11.2 British administrative boundaries (LSOAs, MSOAs and LAs)\n\nAvailability\nThe dataset for the boundaries of the lower-layer super-output areas (LSOAs) within London is stored as a shapefile that can be found under:\n\nst_LSOA &lt;- st_read(\"data/geodemographics-old/LSOA_2011_London_gen_MHW/LSOA_2011_London_gen_MHW.shp\")\n\nReading layer `LSOA_2011_London_gen_MHW' from data source \n  `/Users/carmen/Documents/github/r4ps/data/geodemographics-old/LSOA_2011_London_gen_MHW/LSOA_2011_London_gen_MHW.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 4835 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n\nData for the shapes of the MSOAs must be downloaded from UK’s GeoPortal here. Make sure you download the 2021 version and store it in the .\\data\\machine-learning\\ folder as a file with the .gpkg extension. We have not included the file in the GitHub repo due to its large size. You can load it with st_read and ensure it is in a projection system of choice.\nThe dataset for the boundaries of the local authority distrits (LADs) for the UK is stored as a shapefile that can be found under:\n\nLA_UK &lt;- st_read(\"./data/networks/Local_Authority_Districts_(December_2022)_Boundaries_UK_BFC/LAD_DEC_2022_UK_BFC.shp\")\n\nReading layer `LAD_DEC_2022_UK_BFC' from data source \n  `/Users/carmen/Documents/github/r4ps/data/networks/Local_Authority_Districts_(December_2022)_Boundaries_UK_BFC/LAD_DEC_2022_UK_BFC.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 374 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -116.1928 ymin: 5336.966 xmax: 655653.8 ymax: 1220302\nProjected CRS: OSGB36 / British National Grid\n\n\n\n\nVariables\nFor each of the 4,835 LSOAs, the following characteristics are available:\n\nnames(st_LSOA)\n\n [1] \"LSOA11CD\"  \"LSOA11NM\"  \"MSOA11CD\"  \"MSOA11NM\"  \"LAD11CD\"   \"LAD11NM\"  \n [7] \"RGN11CD\"   \"RGN11NM\"   \"USUALRES\"  \"HHOLDRES\"  \"COMESTRES\" \"POPDEN\"   \n[13] \"HHOLDS\"    \"AVHHOLDSZ\" \"geometry\" \n\n\nwhere:\n\nLSOA11CD: Lower-Layer Super-Output Area code\nLSOA11NM: Lower-Layer Super-Output Area code\nMSOA11CD: Medium-Layer Super-Output Area code\nMSOA11NM: Medium-Layer Super-Output Area code\nLAD11CD: Local Authority District code\nLAD11NM: Local Authority District name\nRGN11CD: Region code\nRGN11NM: Region name\nUSUALRES: Usual residents\nHHOLDRES: Household residents\nCOMESTRES: Communal Establishment residents\nPOPDEN: Population density\nHHOLDS: Number of households\nAVHHOLDSZ: Average household size\ngeometry: Polygon of LSOA\n\nFor each of the 374 LADs, the following characteristics are available:\n\nnames(LA_UK)\n\n [1] \"OBJECTID\"   \"LAD22CD\"    \"LAD22NM\"    \"BNG_E\"      \"BNG_N\"     \n [6] \"LONG\"       \"LAT\"        \"GlobalID\"   \"SHAPE_Leng\" \"SHAPE_Area\"\n[11] \"geometry\"  \n\n\nwhere:\n\nOBJECTID: object identifier\nLAD22CD: Local Authority District code\nLAD22NM: Local Authority District name\nBNG_E: Location Easting\nBNG_N: Location Northing\nLONG: Location Longitude\nLAT: Location Latitude\nGlobalID: Global Identifier\nSHAPE_Leng: Boundary length\nSHAPE_Area: Area within boundary\ngeometry: Polygon of LAD\n\n\n\nProjection\nThe shapes of each LSOA are stored as polygons an expressed in the OSGB36 projection:\n\nst_crs(st_LSOA)\n\nCoordinate Reference System:\n  User input: OSGB36 / British National Grid \n  wkt:\nPROJCRS[\"OSGB36 / British National Grid\",\n    BASEGEOGCRS[\"OSGB36\",\n        DATUM[\"Ordnance Survey of Great Britain 1936\",\n            ELLIPSOID[\"Airy 1830\",6377563.396,299.3249646,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6277]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",49,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",-2,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.999601272,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",400000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",-100000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nSimilarly, the shapes of each LAD are stored as polygons an expressed in the OSGB36 projection:\n\nst_crs(LA_UK)\n\nCoordinate Reference System:\n  User input: OSGB36 / British National Grid \n  wkt:\nPROJCRS[\"OSGB36 / British National Grid\",\n    BASEGEOGCRS[\"OSGB36\",\n        DATUM[\"Ordnance Survey of Great Britain 1936\",\n            ELLIPSOID[\"Airy 1830\",6377563.396,299.3249646,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4277]],\n    CONVERSION[\"British National Grid\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",49,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",-2,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996012717,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",400000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",-100000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"United Kingdom (UK) - offshore to boundary of UKCS within 49°45'N to 61°N and 9°W to 2°E; onshore Great Britain (England, Wales and Scotland). Isle of Man onshore.\"],\n        BBOX[49.75,-9.01,61.01,2.01]],\n    ID[\"EPSG\",27700]]\n\n\n\n\nSource & Pre-processing\nThe boundaries for the LSOAs within London can be found directly from the London Datastore website.\nThe boundaries for the LADs for the UK can be found on the ONS Open Geography Portal website. To filter for the London LADs, i.e. the London boroughs, we run the following line of code:\n\nLND_boroughs &lt;- LA_UK %&gt;% filter(grepl('E09', LAD22CD))",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data sets</span>"
    ]
  },
  {
    "objectID": "data-sets.html#twitter-migration-data-for-the-uk",
    "href": "data-sets.html#twitter-migration-data-for-the-uk",
    "title": "11  Data sets",
    "section": "11.3 Twitter migration data for the UK",
    "text": "11.3 Twitter migration data for the UK\n\n11.3.1 Availability\nThe dataset is stored on a gpkg file that can be found, within the structure of this project, under:\n\nst_LSOA &lt;- st_read(\"./data/networks/internal_migration_uk.csv\")\n\nReading layer `internal_migration_uk' from data source \n  `/Users/carmen/Documents/github/r4ps/data/networks/internal_migration_uk.csv' \n  using driver `CSV'\n\n\nWarning: no simple feature geometries present: returning a data.frame or tbl_df\n\n\n\n\n11.3.2 Source and preprocessing\nThe data was created for the paper (Wang et al. 2022). The paper includes details on the methodology.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data sets</span>"
    ]
  },
  {
    "objectID": "data-sets.html#worldpop-population-count-data-for-ukraine",
    "href": "data-sets.html#worldpop-population-count-data-for-ukraine",
    "title": "11  Data sets",
    "section": "11.4 Worldpop population count data for Ukraine",
    "text": "11.4 Worldpop population count data for Ukraine",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data sets</span>"
    ]
  },
  {
    "objectID": "data-sets.html#census-population-count-data-for-uk",
    "href": "data-sets.html#census-population-count-data-for-uk",
    "title": "11  Data sets",
    "section": "11.5 Census population count data for UK",
    "text": "11.5 Census population count data for UK",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data sets</span>"
    ]
  },
  {
    "objectID": "data-sets.html#ukraines-administrative-boundaries",
    "href": "data-sets.html#ukraines-administrative-boundaries",
    "title": "11  Data sets",
    "section": "11.6 Ukraine’s administrative boundaries",
    "text": "11.6 Ukraine’s administrative boundaries",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data sets</span>"
    ]
  },
  {
    "objectID": "data-sets.html#twitter-data-on-public-opinion-originated-in-the-us-and-in-the-uk",
    "href": "data-sets.html#twitter-data-on-public-opinion-originated-in-the-us-and-in-the-uk",
    "title": "11  Data sets",
    "section": "11.7 Twitter data on public opinion originated in the US and in the UK",
    "text": "11.7 Twitter data on public opinion originated in the US and in the UK",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data sets</span>"
    ]
  },
  {
    "objectID": "data-sets.html#reddit-data",
    "href": "data-sets.html#reddit-data",
    "title": "11  Data sets",
    "section": "11.8 Reddit data",
    "text": "11.8 Reddit data",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data sets</span>"
    ]
  },
  {
    "objectID": "data-sets.html#google-mobility-data-for-italy-and-the-uk",
    "href": "data-sets.html#google-mobility-data-for-italy-and-the-uk",
    "title": "11  Data sets",
    "section": "11.9 Google mobility data for Italy and the UK",
    "text": "11.9 Google mobility data for Italy and the UK",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data sets</span>"
    ]
  },
  {
    "objectID": "data-sets.html#covid-19-cases-data-for-london-and-rome",
    "href": "data-sets.html#covid-19-cases-data-for-london-and-rome",
    "title": "11  Data sets",
    "section": "11.10 COVID-19 cases data for London and Rome",
    "text": "11.10 COVID-19 cases data for London and Rome",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data sets</span>"
    ]
  },
  {
    "objectID": "data-sets.html#census-msoa-data-for-england-and-wales",
    "href": "data-sets.html#census-msoa-data-for-england-and-wales",
    "title": "11  Data sets",
    "section": "11.11 Census MSOA data for England and Wales",
    "text": "11.11 Census MSOA data for England and Wales\n\nAvailability\nThe dataset for the demographic census data of each MSOA in England and Wales can be loaded as a csv file from:\n\ndf_MSOA &lt;- read.csv(\"./data/machine-learning/census2021-msoa.csv\")\n\nA dataset for the data on the median rent price for each MSOA can be loaded as a csv as below. This data is from Zoopla and is made available here for non-commercial use, through the Urban Big Data Centre:\n\ndf_rent &lt;- read.csv(\"./data/machine-learning/zoopla_mean_rent_msoa.csv\")\n\n\n\nVariables\nFor each of the 7,080 MSOAs recorded in England and Wales, the following fields are available:\n\nnames(df_MSOA)\n\n [1] \"X\"                \"date\"             \"geography\"        \"geography.code\"  \n [5] \"inHH\"             \"inCE\"             \"SING\"             \"MARRIED\"         \n [9] \"SEP\"              \"DIV\"              \"WIDOW\"            \"UK\"              \n[13] \"EU\"               \"AFR\"              \"AS\"               \"AM\"              \n[17] \"OC\"               \"BO\"               \"DENSITY\"          \"Y14orUNDER\"      \n[21] \"Y15to19\"          \"Y20to24\"          \"Y25to29\"          \"Y30to34\"         \n[25] \"Y35to49\"          \"Y40to44\"          \"Y45to49\"          \"Y50to54\"         \n[29] \"Y55to59\"          \"Y60to64\"          \"Y65orOVER\"        \"F\"               \n[33] \"M\"                \"HH1\"              \"HH2\"              \"HH3\"             \n[37] \"HH4\"              \"HH5\"              \"HH6\"              \"ADD1YagoSAME\"    \n[41] \"ADD1YagoSTUDENT\"  \"ADD1YagoUK\"       \"ADD1YagoNONUK\"    \"NHH\"             \n[45] \"OWN\"              \"MORTGAGE\"         \"SHAREDOWN\"        \"RENTfromCOUNCIL\" \n[49] \"RENTotherSOCIAL\"  \"RENTprivate\"      \"RENTprivateOTHER\" \"RENTfree\"        \n\n\nFor a description of the variables in the columns of df_MSOA, we can load a dictionary for these variables:\n\ndf_dictionary &lt;- read.csv(\"./data/machine-learning/Dictionary.csv\")\nhead(df_dictionary)\n\n                                      Dictionary       X\n1                                                       \n2                                           Name     Key\n3                 Lives in household (% persons)    inHH\n4    Lives in communal establishment (% persons)    inCE\n5 Never married or civil partnership (% persons)    SING\n6    Married or in civil partnership (% persons) MARRIED\n\n\n\n\nSource & pre-processing\nData on the the census characteristics for different MSOAs can be downloaded from the Nomis website. Data on the average net household income can be obtained from the ONS website.\nData on the median houseprice for different MSOAs can be downloaded from the ONS website.\nAll the data has been pre-processed on Microsoft Excel.\n\n\n\n\nWang, Yikang, Chen Zhong, Qili Gao, and Carmen Cabrera-Arnau. 2022. “Understanding Internal Migration in the UK Before and During the COVID-19 Pandemic Using Twitter Data.” Urban Informatics 1 (1): 15.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data sets</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Abbott, Andrew, and Angela Tsay. 2000. “Sequence Analysis and\nOptimal Matching Methods in Sociology: Review and Prospect.”\nSociological Methods & Research 29 (1): 3–33.\n\n\nArribas-Bel, Dani, Mark Green, Francisco Rowe, and Alex Singleton. 2021.\n“Open Data Products-A Framework for Creating Valuable Analysis\nReady Data.” Journal of Geographical Systems 23 (4):\n497–514. https://doi.org/10.1007/s10109-021-00363-5.\n\n\nBackman, Mikaela, Esteban Lopez, and Francisco Rowe. 2020. “The\nOccupational Trajectories and Outcomes of Forced Migrants in Sweden.\nEntrepreneurship, Employment or Persistent Inactivity?” Small\nBusiness Economics 56 (3): 963–83. https://doi.org/10.1007/s11187-019-00312-z.\n\n\nBail, Christopher A., Lisa P. Argyle, Taylor W. Brown, John P. Bumpus,\nHaohan Chen, M. B. Fallin Hunzaker, Jaemin Lee, Marcus Mann, Friedolin\nMerhout, and Alexander Volfovsky. 2018. “Exposure to Opposing\nViews on Social Media Can Increase Political Polarization.”\nProceedings of the National Academy of Sciences 115 (37):\n9216–21. https://doi.org/10.1073/pnas.1804840115.\n\n\nBar, Michael, Moshe Hazan, Oksana Leukhina, David Weiss, and Hosny\nZoabi. 2018. “Why Did Rich Families Increase Their Fertility?\nInequality and Marketization of Child Care.” Journal of\nEconomic Growth 23: 427–63.\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine\nLearning. Information Science and Statistics. New York: Springer.\n\n\nBlei, David M, Andrew Y Ng, and Michael I Jordan. 2003. “Latent\nDirichlet Allocation.” Journal of Machine Learning\nResearch 3 (Jan): 993–1022.\n\n\nBoehmke, Brad. 2019. “Hands-on Machine Learning with r. Chapter 9:\nDecision Trees.” https://bradleyboehmke.github.io/HOML/DT.html.\n\n\nBreiman, L. 1984. Classification and Regression\nTrees (1st Ed.). Routledge.\n\n\nBrodeur, Abel, Andrew E Clark, Sarah Fleche, and Nattavudh Powdthavee.\n2021. “COVID-19, Lockdowns and Well-Being: Evidence from Google\nTrends.” Journal of Public Economics 193: 104346.\n\n\nCabrera-Arnau, Carmen, Chen Zhong, Michael Batty, Ricardo Silva, and\nSoong Moon Kang. 2022. “Inferring Urban Polycentricity from the\nVariability in Human Mobility Patterns.” arXiv. https://doi.org/10.48550/ARXIV.2212.03973.\n\n\nCadwalladr, Carole, and Emma Graham-Harrison. 2018. “Revealed: 50\nMillion Facebook Profiles Harvested for Cambridge Analytica in Major\nData Breach.” The Guardian 17 (1): 22.\n\n\nCesare, Nina, Hedwig Lee, Tyler McCormick, Emma Spiro, and Emilio\nZagheni. 2018. “Promises and Pitfalls of Using Digital Traces for\nDemographic Research.” Demography 55 (5): 1979–99. https://doi.org/10.1007/s13524-018-0715-2.\n\n\nCheong, Pauline Hope, Rosalind Edwards, Harry Goulbourne, and John\nSolomos. 2007. “Immigration, Social Cohesion and Social Capital: A\nCritical Review.” Critical Social Policy 27 (1): 24–49.\nhttps://doi.org/10.1177/0261018307072206.\n\n\nCinelli, Matteo, Walter Quattrociocchi, Alessandro Galeazzi, Carlo\nMichele Valensise, Emanuele Brugnoli, Ana Lucia Schmidt, Paola Zola,\nFabiana Zollo, and Antonio Scala. 2020. “The COVID-19 Social Media\nInfodemic.” Scientific Reports 10 (1): 1–10.\n\n\nCowper, Andy. 2020. “Covid-19: Are We Getting the Communications\nRight?” BMJ, March, m919. https://doi.org/10.1136/bmj.m919.\n\n\nDolega, Les, Francisco Rowe, and Emma Branagan. 2021. “Going\nDigital? The Impact of Social Media Marketing on Retail Website Traffic,\nOrders and Sales.” Journal of Retailing and Consumer\nServices 60 (May): 102501. https://doi.org/10.1016/j.jretconser.2021.102501.\n\n\nElbagir, Shihab, and Jing Yang. 2020. “Sentiment Analysis on\nTwitter with Python’s Natural Language Toolkit and VADER\nSentiment Analyzer.” IAENG Transactions on Engineering\nSciences, January. https://doi.org/10.1142/9789811215094_0005.\n\n\nEuropean Commision. 2019. “10 Trends Shaping\nMigration.” https://op.europa.eu/en/publication-detail/-/publication/aa25fb8f-10cc-11ea-8c1f-01aa75ed71a1.\n\n\nFielding, A. J. 1992. “Migration and Social Mobility: South East\nEngland as an Escalator Region.” Regional Studies 26\n(1): 1–15. https://doi.org/10.1080/00343409212331346741.\n\n\nFranklin, Rachel. 2022. “Quantitative Methods II: Big\nTheory.” Progress in Human Geography 47 (1): 178–86. https://doi.org/10.1177/03091325221137334.\n\n\nGabadinho, Alexis, Gilbert Ritschard, Nicolas S. Müller, and Matthias\nStuder. 2011. “Analyzing and Visualizing State Sequences\ninRwithTraMineR.”\nJournal of Statistical Software 40 (4). https://doi.org/10.18637/jss.v040.i04.\n\n\nGabadinho, Alexis, Gilbert Ritschard, Matthias Studer, and Nicolas S\nMüller. 2009. “Mining Sequence Data in r with the TraMineR\nPackage: A User’s Guide.” Geneva: Department of Econometrics\nand Laboratory of Demography, University of Geneva.\n\n\nGhani, Norjihan Abdul, Suraya Hamid, Ibrahim Abaker Targio Hashem, and\nEjaz Ahmed. 2019. “Social Media Big Data Analytics: A\nSurvey.” Computers in Human Behavior 101 (December):\n417–28. https://doi.org/10.1016/j.chb.2018.08.039.\n\n\nGonzález-Leonardo, Miguel, Niall Newsham, and Francisco Rowe. 2023.\n“Understanding Population Decline Trajectories in Spain Using\nSequence Analysis.” Geographical Analysis, January. https://doi.org/10.1111/gean.12357.\n\n\nGoodman-Bacon, Andrew, and Jan Marcus. 2020. “Using\nDifference-in-Differences to Identify Causal Effects of COVID-19\nPolicies.”\n\n\nGreen, Mark, Frances Darlington Pollock, and Francisco Rowe. 2021.\n“New Forms of Data and New Forms of Opportunities to Monitor and\nTackle a Pandemic.” In, 423–29. Springer International\nPublishing. https://doi.org/10.1007/978-3-030-70179-6_56.\n\n\nHilbert, Martin, and Priscila López. 2011. “The\nWorld’s Technological Capacity to Store, Communicate, and\nCompute Information.” Science 332 (6025): 60–65. https://doi.org/10.1126/science.1200970.\n\n\nHome Affairs Committee. 2020. “Oral evidence:\nHome Office preparedness for Covid-19 (Coronavirus), HC\n232.” London: House of Commons. https://committees.parliament.uk/oralevidence/359/default/.\n\n\nHutto, C., and Eric Gilbert. 2014. “VADER: A Parsimonious\nRule-Based Model for Sentiment Analysis of Social Media Text.”\nProceedings of the International AAAI Conference on Web and Social\nMedia 8 (1): 216–25. https://doi.org/10.1609/icwsm.v8i1.14550.\n\n\nJoint Research Centre. 2022. Data innovation in demography,\nmigration and human mobility. LU: European Commission. Publications\nOffice. https://doi.org/10.2760/027157.\n\n\nKashyap, Ridhi, R. Gordon Rinderknecht, Aliakbar Akbaritabar, Diego\nAlburez-Gutierrez, Sofia Gil-Clavel, André Grow, Jisu Kim, et al. 2022.\n“Digital and Computational Demography.” http://dx.doi.org/10.31235/osf.io/7bvpt.\n\n\nKaufman, Leonard, and Peter J Rousseeuw. 2009. Finding Groups in\nData: An Introduction to Cluster Analysis. John Wiley & Sons.\n\n\nKitchin, Rob. 2014. “Big Data, New Epistemologies and Paradigm\nShifts.” Big Data & Society 1 (1): 205395171452848.\nhttps://doi.org/10.1177/2053951714528481.\n\n\nLazer, David, Alex Pentland, Lada Adamic, Sinan Aral, Albert-László\nBarabási, Devon Brewer, Nicholas Christakis, et al. 2009.\n“Computational Social Science.” Science 323\n(5915): 721–23. https://doi.org/10.1126/science.1167742.\n\n\nLazer, David, Alex Pentland, Duncan J. Watts, Sinan Aral, Susan Athey,\nNoshir Contractor, Deen Freelon, et al. 2020. “Computational\nSocial Science: Obstacles and Opportunities.” Science\n369 (6507): 1060–62. https://doi.org/10.1126/science.aaz8170.\n\n\nLiang, Hai, and King-wa Fu. 2015. “Testing Propositions Derived\nfrom Twitter Studies: Generalization and Replication in Computational\nSocial Science.” Edited by Zi-Ke Zhang. PLOS ONE 10 (8):\ne0134270. https://doi.org/10.1371/journal.pone.0134270.\n\n\nMarshall, Emily A. 2013. “Defining Population Problems: Using\nTopic Models for Cross-National Comparison of Disciplinary\nDevelopment.” Poetics 41 (6): 701–24.\n\n\nNewman, Mark. 2018. Networks / Mark Newman. Second edition.\nOxford: Oxford University Press.\n\n\nNewsham, Niall, and Francisco Rowe. 2022a. “Understanding the\nTrajectories of Population Decline Across Rural and Urban Europe: A\nSequence Analysis.” https://doi.org/10.48550/ARXIV.2203.09798.\n\n\n———. 2022b. “Understanding Trajectories of Population Decline\nAcross Rural and Urban Europe: A Sequence Analysis.”\nPopulation, Space and Place, December. https://doi.org/10.1002/psp.2630.\n\n\nOgnyanova, K. 2016. “Network Analysis with r and Igraph: NetSci x\nTutorial.” www.kateto.net/networks-r-igraph.\n\n\nPatias, Nikos, Francisco Rowe, and Dani Arribas-Bel. 2021.\n“Trajectories of Neighbourhood Inequality in Britain: Unpacking\nInter-Regional Socioeconomic Imbalances,\n1971-2011.” The Geographical Journal 188\n(2): 150–65. https://doi.org/10.1111/geoj.12420.\n\n\nPatias, Nikos, Francisco Rowe, Stefano Cavazzi, and Dani Arribas-Bel.\n2021. “Sustainable Urban Development Indicators in Great Britain\nfrom 2001 to 2016.” Landscape and Urban Planning 214\n(October): 104148. https://doi.org/10.1016/j.landurbplan.2021.104148.\n\n\nPetti, Samantha, and Abraham Flaxman. 2020. “Differential Privacy\nin the 2020 US Census: What Will It Do? Quantifying the Accuracy/Privacy\nTradeoff.” Gates Open Research 3 (April): 1722. https://doi.org/10.12688/gatesopenres.13089.2.\n\n\nPrieto Curiel, Rafael, Carmen Cabrera-Arnau, and Steven Richard Bishop.\n2022. “Scaling Beyond Cities.” Frontiers in\nPhysics 10. https://doi.org/10.3389/fphy.2022.858307.\n\n\nRosa, H., N. Pereira, R. Ribeiro, P. C. Ferreira, J. P. Carvalho, S.\nOliveira, L. Coheur, P. Paulino, A. M. Veiga Simão, and I. Trancoso.\n2019. “Automatic Cyberbullying Detection: A Systematic\nReview.” Computers in Human Behavior 93 (April): 333–45.\nhttps://doi.org/10.1016/j.chb.2018.12.021.\n\n\nRowe, Francisco. 2021a. “Using Twitter Data to Monitor Immigration\nSentiment.” http://dx.doi.org/10.31219/osf.io/sf7u4.\n\n\n———. 2021b. “Big Data and Human Geography.” http://dx.doi.org/10.31235/osf.io/phz3e.\n\n\n———. 2022a. “Introduction to Geographic Data Science.”\nOpen Science Framework, August. https://doi.org/10.17605/OSF.IO/VHY2P.\n\n\n———. 2022b. “Using Digital Footprint Data to Monitor Human\nMobility and Support Rapid Humanitarian Responses.” Regional\nStudies, Regional Science 9 (1): 665–68. https://doi.org/10.1080/21681376.2022.2135458.\n\n\nRowe, Francisco, and Dani Arribas-Bel. 2022. “Spatial Modelling\nfor Data Scientists.” Open Science Framework, August. https://doi.org/10.17605/OSF.IO/8F6XR.\n\n\nRowe, Francisco, Jonathan Corcoran, and Martin Bell. 2016. “The\nReturns to Migration and Human Capital Accumulation Pathways:\nNon-Metropolitan Youth in the School-to-Work Transition.” The\nAnnals of Regional Science 59 (3): 819–45. https://doi.org/10.1007/s00168-016-0771-8.\n\n\nRowe, Francisco, Michael Mahony, Eduardo Graells-Garrido, Marzia Rango,\nand Niklas Sievers. 2021. “Using Twitter to Track Immigration\nSentiment During Early Stages of the COVID-19 Pandemic.” Data\n& Policy 3. https://doi.org/10.1017/dap.2021.38.\n\n\nRowe, Francisco, Michael Mahony, Niklas Sievers, Marzia Rango, and\nEduardo Graells-Garrido. 2021. “Sentiment\ntowards Migration during COVID-19. What Twitter Data Can Tell\nUs.” IOM Publications.\n\n\nRowe, Francisco, Ruth Neville, and Miguel González-Leonardo. 2022.\n“Sensing Population Displacement from Ukraine Using Facebook Data:\nPotential Impacts and Settlement Areas.” http://dx.doi.org/10.31219/osf.io/7n6wm.\n\n\nSalmela-Aro, Katariina, Noona Kiuru, Jari-Erik Nurmi, and Mervi Eerola.\n2011. “Mapping Pathways to Adulthood Among Finnish University\nStudents: Sequences, Patterns, Variations in Family- and Work-Related\nRoles.” Advances in Life Course Research 16 (1): 25–41.\nhttps://doi.org/10.1016/j.alcr.2011.01.003.\n\n\nSchleicher, Andreas. 2020. “The Impact of COVID-19 on Education:\nInsights from\" Education at a Glance 2020\".” OECD\nPublishing.\n\n\nSielge, Jullia, and David Robinson. 2022. Welcome to Text Mining\nwith r. O’Reilly. https://www.tidytextmining.com.\n\n\nSingleton, Alex, and Daniel Arribas-Bel. 2019. “Geographic Data\nScience.” Geographical Analysis 53 (1): 61–75. https://doi.org/10.1111/gean.12194.\n\n\n“Stop the Coronavirus Stigma Now.” 2020. Nature\n580 (7802): 165–65. https://doi.org/10.1038/d41586-020-01009-0.\n\n\nTatem, Andrew J. 2017. “WorldPop, Open Data for Spatial\nDemography.” Scientific Data 4 (1). https://doi.org/10.1038/sdata.2017.4.\n\n\nTurok, Ivan, and Vlad Mykhnenko. 2007. “The Trajectories of\nEuropean Cities, 19602005.” Cities 24 (3):\n165–82. https://doi.org/10.1016/j.cities.2007.01.007.\n\n\nUgolini, Francesca, Luciano Massetti, Pedro Calaza-Martı́nez, Paloma\nCariñanos, Cynnamon Dobbs, Silvija Krajter Ostoić, Ana Marija Marin, et\nal. 2020. “Effects of the COVID-19 Pandemic on the Use and\nPerceptions of Urban Green Space: An International Exploratory\nStudy.” Urban Forestry & Urban Greening 56: 126888.\n\n\nWang, Yikang, Chen Zhong, Qili Gao, and Carmen Cabrera-Arnau. 2022.\n“Understanding Internal Migration in the UK Before and During the\nCOVID-19 Pandemic Using Twitter Data.” Urban Informatics\n1 (1): 15.\n\n\nZagheni, Emilio, and Ingmar Weber. 2015. “Demographic Research\nwith Non-Representative Internet Data.” Edited by Nikolaos\nAskitas and Professor Professor Klaus F. Zimmermann. International\nJournal of Manpower 36 (1): 13–25. https://doi.org/10.1108/ijm-12-2014-0261.\n\n\nZhou, Muzhi, and Man-Yee Kan. 2021. “The Varying Impacts of\nCOVID-19 and Its Related Measures in the UK: A Year in Review.”\nPLoS One 16 (9): e0257286.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "longitudinal-1.html",
    "href": "longitudinal-1.html",
    "title": "8  Modelling Time",
    "section": "",
    "text": "8.1 Dependencies\n# Data\nlibrary(sf)\nlibrary(readr)\nlibrary(tidyverse)\n\n# Dates and Times\nlibrary(lubridate)\n\n# Regression Spline Functions and Classes\nlibrary(splines)\n\n# graphs\nlibrary(ggplot2)\nlibrary(dygraphs)\nlibrary(plotly)\nlibrary(ggthemes)\nlibrary(ggpmisc)\nlibrary(gridExtra)\nlibrary(viridis)\nlibrary(ggformula)\nlibrary(ggimage)\nlibrary(grid)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling Time</span>"
    ]
  },
  {
    "objectID": "longitudinal-1.html#data",
    "href": "longitudinal-1.html#data",
    "title": "8  Modelling Time",
    "section": "8.2 Data",
    "text": "8.2 Data\nWe will use a sample of Google Mobility data. Google has made available Community Mobility data to provide insights into what changed in response to policies aimed at combating COVID-19. The data also has accompanying reports which analyse movement trends over time by geography, across different categories of places such as retail and recreation, groceries and pharmacies, parks, transit stations, workplaces, and residential. Data is available from 2020 and 2022. Community Mobility data is no longer being updated as of 2022-10-15. All historical data will remain publicly available.\nLocation accuracy and the understanding of categorised places varies from region to region, so it is not recommend the data is used to compare changes between countries, or between regions with different characteristics (e.g. rural versus urban areas). Region that do not have statistically significant levels of data have been left out of the report.\nSome important facts about the data:\n\nChanges for each day are compared to a baseline value for that day of the week. The baseline is the median value.\nThe data that is included in the calculation depends on user settings, connectivity and whether it meets our privacy threshold.\nIf the privacy threshold isn’t met (when somewhere isn’t busy enough to ensure anonymity) a change for the day is not shown.\n\nFor more about this data please see here.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling Time</span>"
    ]
  },
  {
    "objectID": "longitudinal-1.html#modelling-time",
    "href": "longitudinal-1.html#modelling-time",
    "title": "8  Modelling Time",
    "section": "8.3 Modelling Time",
    "text": "8.3 Modelling Time\nFirst we import the data we will be working with. We will be looking at Google Mobility data for Italy.\n\nmobility_it &lt;- read.csv(\"data/longitudinal-1/2022_IT_Region_Mobility_Report.csv\", header = TRUE)\n\nCheck out variables in the dataframe\n\n# Check out variables in the dataframe \ncolnames(mobility_it)\n\n [1] \"country_region_code\"                               \n [2] \"country_region\"                                    \n [3] \"sub_region_1\"                                      \n [4] \"sub_region_2\"                                      \n [5] \"metro_area\"                                        \n [6] \"iso_3166_2_code\"                                   \n [7] \"census_fips_code\"                                  \n [8] \"place_id\"                                          \n [9] \"date\"                                              \n[10] \"retail_and_recreation_percent_change_from_baseline\"\n[11] \"grocery_and_pharmacy_percent_change_from_baseline\" \n[12] \"parks_percent_change_from_baseline\"                \n[13] \"transit_stations_percent_change_from_baseline\"     \n[14] \"workplaces_percent_change_from_baseline\"           \n[15] \"residential_percent_change_from_baseline\"          \n\n\nLong data vs. Wide data\nLong data and wide data are two different formats used to store and organize data in a tabular form. Wide data is a format where each variable is stored in a separate column, and each observation or time point is stored in a separate row. This format is often useful when the number of variables is small compared to the number of observations, and is often used for descriptive statistics and exploratory data analysis.\n\n\n        dates           A          B\n1  2022-01-01 -0.56047565  1.2240818\n2  2022-01-02 -0.23017749  0.3598138\n3  2022-01-03  1.55870831  0.4007715\n4  2022-01-04  0.07050839  0.1106827\n5  2022-01-05  0.12928774 -0.5558411\n6  2022-01-06  1.71506499  1.7869131\n7  2022-01-07  0.46091621  0.4978505\n8  2022-01-08 -1.26506123 -1.9666172\n9  2022-01-09 -0.68685285  0.7013559\n10 2022-01-10 -0.44566197 -0.4727914\n\n\nLong data, on the other hand, is a format where each variable is represented by two or more columns: one column for the variable name and another for the variable values. This format is often useful when we have many variables or when we want to perform statistical analysis.\n\n\n        dates series       value\n1  2022-01-01      A -0.56047565\n2  2022-01-01      B  1.22408180\n3  2022-01-02      A -0.23017749\n4  2022-01-02      B  0.35981383\n5  2022-01-03      A  1.55870831\n6  2022-01-03      B  0.40077145\n7  2022-01-04      A  0.07050839\n8  2022-01-04      B  0.11068272\n9  2022-01-05      A  0.12928774\n10 2022-01-05      B -0.55584113\n11 2022-01-06      A  1.71506499\n12 2022-01-06      B  1.78691314\n13 2022-01-07      A  0.46091621\n14 2022-01-07      B  0.49785048\n15 2022-01-08      A -1.26506123\n16 2022-01-08      B -1.96661716\n17 2022-01-09      A -0.68685285\n18 2022-01-09      B  0.70135590\n19 2022-01-10      A -0.44566197\n20 2022-01-10      B -0.47279141\n\n\nBoth wide and long data formats have their own advantages and disadvantages, and the choice between them often depends on the specific data and the analysis that is planned. Transforming data from one format to another is a common task in data processing and analysis, and can be accomplished using various data manipulation tools in R, such as tidyr and reshape2 packages. Look back at the Italian google mobility data to check which format it is in.\nDate format\nTime series aim to study the evolution of one or several variables through time. Several packages that are part of the tidyverse family will help you analyse time series data in R. The lubridatepackage is your best friend to deal with the date format. ggplot2 will allow you to plot it efficiently. dygraphs will also help build attractive interactive charts.\nBuilding time series requires the time variable to be at the date format. The first step of your analysis must be to double check that R read your data correctly, i.e. at the date format. This is possible thanks to the str() function:\n\n# Checking date format\nstr(mobility_it)\n\n'data.frame':   36576 obs. of  15 variables:\n $ country_region_code                               : chr  \"IT\" \"IT\" \"IT\" \"IT\" ...\n $ country_region                                    : chr  \"Italy\" \"Italy\" \"Italy\" \"Italy\" ...\n $ sub_region_1                                      : chr  \"ALL\" \"ALL\" \"ALL\" \"ALL\" ...\n $ sub_region_2                                      : chr  \"\" \"\" \"\" \"\" ...\n $ metro_area                                        : logi  NA NA NA NA NA NA ...\n $ iso_3166_2_code                                   : chr  \"\" \"\" \"\" \"\" ...\n $ census_fips_code                                  : logi  NA NA NA NA NA NA ...\n $ place_id                                          : chr  \"ChIJA9KNRIL-1BIRb15jJFz1LOI\" \"ChIJA9KNRIL-1BIRb15jJFz1LOI\" \"ChIJA9KNRIL-1BIRb15jJFz1LOI\" \"ChIJA9KNRIL-1BIRb15jJFz1LOI\" ...\n $ date                                              : chr  \"01/01/2022\" \"02/01/2022\" \"03/01/2022\" \"04/01/2022\" ...\n $ retail_and_recreation_percent_change_from_baseline: int  -65 -27 -13 -13 -10 -30 -20 -27 -39 -22 ...\n $ grocery_and_pharmacy_percent_change_from_baseline : int  -80 7 28 30 42 -24 25 6 -7 19 ...\n $ parks_percent_change_from_baseline                : int  21 12 19 17 10 42 13 3 -36 -18 ...\n $ transit_stations_percent_change_from_baseline     : int  -49 -18 -36 -37 -37 -50 -38 -30 -34 -35 ...\n $ workplaces_percent_change_from_baseline           : int  -69 -13 -42 -41 -42 -78 -48 -27 -13 -21 ...\n $ residential_percent_change_from_baseline          : int  13 6 13 13 12 23 16 9 10 10 ...\n\n\nThis is already looking fine in the google mobility data, but in many other cases the lubridate package is such a life saver. It offers several function which name are composed by 3 letters: year (y), month (m) and day (d). Have a look at lubridate cheat sheet to see more about converting time variables to useful formats. There is also the anytime() function in the anytime package whose sole goal is to automatically parse strings as dates regardless of the format.\n\n#Convert the date column to a date format using the dmy() function \nmobility_it$date_fix &lt;- dmy(mobility_it$date)  \n\nTidy up some variables.\n\n# Rename variables\nmobility_it &lt;- mobility_it %&gt;%\n  rename(grocery_pharmacy = grocery_and_pharmacy_percent_change_from_baseline,\n         parks_percent_change_from_baseline = parks_percent_change_from_baseline,\n         transit = transit_stations_percent_change_from_baseline)\n\nInitial time-series plotting with ggplot2\nLet’s start easy by keeping only data for the whole of Italy.\n\n# Filter the dataframe to keep only the rows for all of Italy\nmobility_it_nogeo &lt;-filter(mobility_it, sub_region_1 == \"ALL\")\n\nggplot2 offers great features when it comes to visualize time series. The date format will be recognized automatically, resulting in a neat x axis labels.\n\n# Most basic bubble plot - uncomment to visualise\n#timeseries1 &lt;- ggplot(data = mobility_it_nogeo, aes(x=date_fix, y=grocery_pharmacy)) + geom_point()\n#timeseries1 \n\n# Adding lines and starting to clean up graph\ntimeseries2 &lt;- ggplot(data = mobility_it_nogeo, aes(x=date_fix, y=grocery_pharmacy)) +\n  geom_point(color=\"#061685\", shape=1) +\n  geom_line(color=\"#2c7bb6\") + \n  theme_tufte() +\n  xlab(\"\") +\n  ylab(\"Grocery and Pharmacy % change\") \n\ntimeseries2\n\n\n\n\n\n\n\n\nThe ggplot2 package recognizes the date format and automatically uses a specific type of X axis. If the time variable isn’t at the date format, this won’t work. Always check with str(data) how variables are understood by R. The scale_x_data() makes it a breeze to customize those labels.\n\n# Use the limit option of the scale_x_date() function to select a time frame in the data:\ntimeseries3 &lt;- ggplot(data = mobility_it_nogeo, aes(x=date_fix, y=grocery_pharmacy)) +\n  geom_point(color=\"#061685\", shape=1) +\n  geom_line( color=\"#2c7bb6\") +\n  xlab(\"\") +\n  ylab(\"Grocery and Pharmacy % change visits\") +\n  theme_tufte() +\n  scale_x_date(limit=c(as.Date(\"2022-01-01\"),as.Date(\"2022-04-01\"))) # Limiting between two dates \n\ntimeseries3\n\n\n\n\n\n\n\n\nplotly is also great to turn the resulting chart interactive in one more line of code. With the ggplotly() function you can hover circles to get a tooltip, or select an area of interest for zooming. You can zoom by selecting an area of interest. Hover over the line to get exact time and value. Export to a widget with htmlwidgets.\n\n# Usual chart\ntimeseries3bis &lt;- ggplot(data = mobility_it_nogeo, aes(x=date_fix, y=grocery_pharmacy)) +\n  geom_point(color=\"#061685\", shape=1) +\n  geom_line(color=\"#2c7bb6\") + \n  theme_tufte() +\n  xlab(\"\") +\n  ylab(\"Grocery and Pharmacy % change\") \n\n# Turn it interactive with ggplotly\ntimeseries_interactive &lt;- ggplotly(timeseries3bis)\ntimeseries_interactive\n\n\n\n\n# Of course this needs more cleaning to be a final output\n\n# To save the widget use the library(htmlwidgets)\n# saveWidget(p, file=paste0( getwd(), \"/HtmlWidget/ggplotlyAreachart.html\"))\n\nLinear trends\nNow let’s try to model trends in the data. Let’s first consider linear trends. Linear trends are the simplest way to model time as a quantitative variable. We can represent time as a series of evenly spaced points on a graph, and then fit a straight line to those points. The slope of the line tells us the rate at which the variable is changing over time. For example, if we are measuring population counts over time, a positive slope would indicate that a population increase, and a negative slope would indicate a population decrease.\n\n# Estimate linear regression model\nlinear_model &lt;- lm(grocery_pharmacy ~ date_fix, mobility_it_nogeo)\n\n# A simple plot line\nplot(mobility_it_nogeo$date_fix,                       \n     mobility_it_nogeo$grocery_pharmacy,\n     type = \"l\")\nlines(mobility_it_nogeo$date_fix,\n      predict(linear_model),\n      col = 2,\n      lwd = 2)\n\n\n\n\n\n\n\n# Extract coefficients of model and print\nmy_coef &lt;- coef(linear_model)            \nmy_coef                            \n\n  (Intercept)      date_fix \n-694.27809786    0.03706687 \n\n# Extract equation of model and print\nmy_equation &lt;- paste(\"y =\",        \n                     coef(linear_model)[[1]],\n                     \"+\",\n                     coef(linear_model)[[2]],\n                     \"* x\")\nmy_equation                        \n\n[1] \"y = -694.278097860568 + 0.037066871224827 * x\"\n\n\nLet’s plot this with ggplot to fit our line through our points. We can use the package ggpmisc to add the equation and R2 with stat_poly_line() and stat_poly_eq.\n\n# Using geom_smooth() for the linear fit\ntimeseries4a &lt;- ggplot(data = mobility_it_nogeo, aes(x=date_fix, y=grocery_pharmacy)) +\n  geom_point(color=\"#061685\", shape=1, alpha = 0.8) +\n  geom_smooth(formula = y ~ x, method=\"lm\") + # linear regression \n  xlab(\"\") +\n  ylab(\"Grocery & Pharmacy %\") +\n  stat_poly_line() +\n  stat_poly_eq(use_label(c(\"eq\", \"R2\"))) +\n theme_tufte() +\n  ggtitle(expression(paste(Delta, \"% Grocery & Pharmacy\")))\n  \ntimeseries4b &lt;- ggplot(data = mobility_it_nogeo, aes(x=date_fix, y=transit)) +\n  geom_point(color=\"#061685\", shape=1, alpha = 0.8) +\n  geom_smooth(formula = y ~ x, method=\"lm\") + #This is where your linear regression is\n  xlab(\"\") +\n  ylab(\"Transit stations %\") +\n  stat_poly_line() +\n  stat_poly_eq(use_label(c(\"eq\", \"R2\"))) +\n theme_tufte() +\n  ggtitle(expression(paste(Delta, \"% Transit stations\")))\n\ntimeseries4c &lt;- ggplot(data = mobility_it_nogeo, aes(x=date_fix, y=parks_percent_change_from_baseline)) +\n  geom_point(color=\"#061685\", shape=1, alpha = 0.8) +\n  geom_smooth(formula = y ~ x, method=\"lm\") + #This is where your linear regression is\n  xlab(\"\") +\n  ylab(\"Parks %\") +\n  stat_poly_line() +\n  stat_poly_eq(use_label(c(\"eq\", \"R2\"))) +\n theme_tufte() +\n  ggtitle(expression(paste(Delta, \"% Visit Parks\")))\n  \n  \ngrid.arrange(timeseries4a, timeseries4b, timeseries4c, ncol=3)\n\n\n\n\n\n\n\n\nAlternatively you can use the ols() function aswell followed by the stat_function(). See here for more details.\nLinear trends have some limitations. In many cases, the relationship between time and the variable we are measuring is not linear, and fitting a straight line may not capture the true nature of the relationship. In such cases, we may need to consider a quadratic trend.\nQuadratic trends\nQuadratic trends model time as a second-degree polynomial, which allows for a curved relationship between time and the variable we are measuring. Quadratic trends can capture more complex patterns in the data than linear trends, such as a gradual increase followed by a gradual decrease. For example, if we are measuring the number of COVID-19 cases over time, a quadratic trend may capture the initial exponential growth, followed by a flattening out of the curve.\nHowever, quadratic trends also have some limitations. They assume that the relationship between time and the variable we are measuring is symmetrical, which may not always be the case. In addition, they can be difficult to interpret, as the coefficient for the quadratic term does not have a straightforward interpretation.\n\ntimeseries5 &lt;- ggplot(data = mobility_it_nogeo, aes(x=date_fix, y=transit)) +\n  geom_point(color=\"#061685\", shape=1, alpha = 0.8) + \n  geom_smooth(formula = y ~ poly(x,2), method=\"lm\", se = T, level = 0.99) + # 2nd order polynomial & adjusting level of the confidence interval\n  xlab(\"\") +\n  ylab(\"Transit stations % change\") +\n theme_tufte() \n  \ntimeseries5\n\n\n\n\n\n\n\ntimeseries6 &lt;- ggplot(data = mobility_it_nogeo, aes(x=date_fix, y=transit)) +\n  geom_point(color=\"#061685\", shape=1, alpha = 0.8) + \n  geom_smooth(formula = y ~ poly(x,3), method=\"lm\", se = T, level = 0.99) + # 3rd order polynomial & adjusting level of the confidence interval\n  xlab(\"\") +\n  ylab(\"Transit stations % change\") +\n theme_tufte() \n  \ntimeseries6\n\n\n\n\n\n\n\n\nSplines\nFinally, we have splines. Splines are a more flexible way to model time as a quantitative variable. Splines allow us to fit a piecewise function to the data, where the function is a series of connected polynomial segments. Each segment captures a different part of the relationship between time and the variable we are measuring. Splines can capture more complex patterns in the data than linear or quadratic trends, and they can be customized to fit the specific shape of the relationship we are trying to model.\nHowever, splines also have some limitations. They can be computationally intensive, and the choice of the number and location of the knots (i.e., the points where the polynomial segments connect) can have a big impact on the results. Splines are useful when the underlying function is complex or unknown, and can be used for a variety of applications, including curve fitting, data smoothing, and prediction.\nIn R, you can plot x vs y using the plot() function. To plot a spline, you can use the spline() function to generate the points for the curve, and then plot the curve using the lines() function.\n\ntimeseries7 &lt;- ggplot(data = mobility_it_nogeo, aes(x=date_fix, y=grocery_pharmacy)) +\n  geom_point(color=\"#061685\", shape=1, alpha = 0.8) +\n  ggformula::stat_spline() + # spline\n  xlab(\"\") +\n  ylab(\"Grocery & Pharmacy %\") +\n theme_tufte() +\n  ggtitle(expression(paste(Delta, \"% Visit Parks\")))\n  \ntimeseries7\n\n\n\n\n\n\n\n\nLinear trends, quadratic trends, and splines are all ways to model time as a quantitative variable, each with their own strengths and weaknesses. The choice of method will depend on the specific research question and the shape of the relationship between time and the variable we are measuring. You can find more on descriptive timeseries analysis here.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling Time</span>"
    ]
  },
  {
    "objectID": "longitudinal-1.html#modelling-time-and-space",
    "href": "longitudinal-1.html#modelling-time-and-space",
    "title": "8  Modelling Time",
    "section": "8.4 Modelling Time and Space",
    "text": "8.4 Modelling Time and Space\nWe can also examine the heterogeneity in the data by region. Some regions in Italy were had many more COVID-19 cases than others.\n\n# Mobility data by region over time\ntimeseries_all_1 &lt;- ggplot(data = mobility_it, aes(x=date_fix, y=transit, color = sub_region_1)) +\n  # geom_point(alpha = 0.8) + \n  geom_smooth(method=\"lm\",formula = y ~ poly(x,2), se=F) +\n  theme(\n  legend.position = \"bottom\",\n  panel.background = element_rect(fill = NA)) +\n  xlab(\"\") +\n  ylab(\"\") +\n  ggtitle(\"Change in use of transit stations\") \n\n# Initial Graph\ntimeseries_all_1  \n\n\n\n\n\n\n\nmobility_it_filtered &lt;- mobility_it %&gt;% \n  filter(sub_region_1 != \"ALL\")\n\n# Mobility data by region over time with some edits\ntimeseries_all_2 &lt;- ggplot(data = mobility_it_filtered, aes(x = date_fix, y = transit, color = sub_region_1)) +\n  geom_smooth(method = \"lm\", formula = y ~ poly(x, 2), se = F, size = 1.2) +\n  scale_color_viridis(discrete = TRUE, option=\"mako\") +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    plot.title = element_text(size = 14, hjust = 0.5, face = \"bold\"),\n    axis.text = element_text(size = 10),\n    axis.title = element_text(size = 12, face = \"bold\"),\n    legend.text = element_text(size = 10),\n    legend.title = element_text(size = 12, face = \"bold\")\n  ) +\n  labs(\n    x = \"\",\n    y = \"\",\n    title = \"Change in use of transit stations\",\n    color = \"Region\"\n  )\n\ntimeseries_all_2\n\n\n\n\n\n\n\n\nThese graphs would need further work. We could for example divide the data between North, Centre and Southern Italy. Let’s load the geojson of italian regions and plot the data.\n\n# Add the polygons of Italy to the environment\nitaly_iso3166 &lt;- st_read(\"data/longitudinal-1/italy_projected_simplified.geojson\")\n\nReading layer `italy_projected_simplified' from data source \n  `/Users/carmen/Documents/github/css/data/longitudinal-1/italy_projected_simplified.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 124 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 1313364 ymin: 3933695 xmax: 2312062 ymax: 5220353\nProjected CRS: Monte Mario / Italy zone 1\n\n# create a simple plot\nplot(italy_iso3166$geometry)\n\n\n\n\n\n\n\n\nNow let’s join the data by regional code.\n\n# Join by regional code\nitaly_iso3166_tidy &lt;- italy_iso3166 %&gt;%\n  left_join(mobility_it, by=c(\"ISO3166.2\"=\"iso_3166_2_code\"))\n\n# check the first rows of the merged data table\n# head(italy_iso3166_tidy)\n\nTo start with, we can plot the data statically using ggplot.\n\n# One point in time\nitaly_sf_subset_1 &lt;- italy_iso3166_tidy %&gt;% filter(date_fix == \"2022-01-01\")\nitaly_sf_subset_2 &lt;- italy_iso3166_tidy %&gt;% filter(date_fix == \"2022-02-01\")\nitaly_sf_subset_3 &lt;- italy_iso3166_tidy %&gt;% filter(date_fix == \"2022-03-01\")\nitaly_sf_subset_4 &lt;- italy_iso3166_tidy %&gt;% filter(date_fix == \"2022-04-01\")\nitaly_sf_subset_5 &lt;- italy_iso3166_tidy %&gt;% filter(date_fix == \"2022-05-01\")\nitaly_sf_subset_6 &lt;- italy_iso3166_tidy %&gt;% filter(date_fix == \"2022-06-01\")\n\n#This may take some time to load\ng1 &lt;- ggplot() +\n  geom_sf(data = italy_sf_subset_1, aes(fill = transit, geometry = geometry)) +\n  scale_fill_gradient2(low = \"#d7191c\", mid = \"white\", high = \"darkblue\", \n                      midpoint = 0, guide = \"colourbar\") +\n  theme_void()\n\n \ng2 &lt;- ggplot() +\n  geom_sf(data = italy_sf_subset_2, aes(fill = transit, geometry = geometry)) +\n    scale_fill_gradient2(low = \"#d7191c\", mid = \"white\", high = \"darkblue\", \n                      midpoint = 0, guide = \"colourbar\") +\n  theme_void()\n\ng3 &lt;- ggplot() +\n  geom_sf(data = italy_sf_subset_3, aes(fill = transit, geometry = geometry)) +\n    scale_fill_gradient2(low = \"#d7191c\", mid = \"white\", high = \"darkblue\", \n                      midpoint = 0, guide = \"colourbar\") +\n  theme_void()\n\ng4 &lt;- ggplot() +\n  geom_sf(data = italy_sf_subset_4, aes(fill = transit, geometry = geometry)) +\n    scale_fill_gradient2(low = \"#d7191c\", mid = \"white\", high = \"darkblue\", \n                      midpoint = 0, guide = \"colourbar\") +\n  theme_void()\n\ng5 &lt;- ggplot() +\n  geom_sf(data = italy_sf_subset_5, aes(fill = transit, geometry = geometry)) +\n    scale_fill_gradient2(low = \"#d7191c\", mid = \"white\", high = \"darkblue\", \n                      midpoint = 0, guide = \"colourbar\") +\n  theme_void()\n\ng6 &lt;- ggplot() +\n  geom_sf(data = italy_sf_subset_6, aes(fill = transit, geometry = geometry)) +\n    scale_fill_gradient2(low = \"#d7191c\", mid = \"white\", high = \"darkblue\", \n                      midpoint = 0, guide = \"colourbar\") +\n  theme_void()\n\ngrid.arrange(g1, g2, g3, g4, g5, g6, ncol = 3, \n             top = textGrob(\"Change in use of transit by Italian Region\", gp=gpar(fontsize=18,font=2)))\n\n\n\n\n\n\n\n\nPlotting time and space interactively is more complex. There are quite a few examples of how to visualise data over space and time here.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling Time</span>"
    ]
  },
  {
    "objectID": "longitudinal-1.html#questions",
    "href": "longitudinal-1.html#questions",
    "title": "8  Modelling Time",
    "section": "8.5 Questions",
    "text": "8.5 Questions\nFor the assignment, we will continue to focus on the United Kingdom as our geographical area of analysis. For this section, we will use Google Mobility data for the UK for 2021. It has the same format as the Google Mobility data for Italy used in this chapter. During this year the UK underwent a third national lockdown, limitations of gatherings and more. For details on the timeline you can have a look here.\nStart by loading both the csv and geojsons.\n\nmobility_gb &lt;- read.csv(\"data/longitudinal-1/2021_GB_Region_Mobility_Report.csv\", header = TRUE)\n\nuk_iso3166 &lt;- st_read(\"data/longitudinal-1/uk_projected_simplified.geojson\")\n\nReading layer `uk_projected_simplified' from data source \n  `/Users/carmen/Documents/github/css/data/longitudinal-1/uk_projected_simplified.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 245 features and 4 fields (with 16 geometries empty)\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -50462.93 ymin: 5270.466 xmax: 655975.3 ymax: 1219809\nProjected CRS: OSGB36 / British National Grid\n\n\n\nChose three variables to focus on and plot them together using ggplot. Discuss what format the data and if you can see any shocks over time by referring to the UK COVID-19 timeline.\nChose one variable and fit a linear, quadratic or spline model. Justify why you think this is the most appropriate and how you would interpret the resulting equation.\nModel both time and space: use ggplot to create a graph of the data by region or other geographical area. You can chose what to focus on here and do not necessarily need to use all the geographical classifications available to you. Analyse the trends that appear in your plot.\nCreate a static or interactive map (bonus points) of your choice from the data.\n\nAnalyse and discuss what insights you obtain into people’s attitudes, concerns, and behaviors during the pandemic, as well as their response to interventions and policies.\n\n\n\n\nCinelli, Matteo, Walter Quattrociocchi, Alessandro Galeazzi, Carlo Michele Valensise, Emanuele Brugnoli, Ana Lucia Schmidt, Paola Zola, Fabiana Zollo, and Antonio Scala. 2020. “The COVID-19 Social Media Infodemic.” Scientific Reports 10 (1): 1–10.\n\n\nSchleicher, Andreas. 2020. “The Impact of COVID-19 on Education: Insights from\" Education at a Glance 2020\".” OECD Publishing.\n\n\nUgolini, Francesca, Luciano Massetti, Pedro Calaza-Martı́nez, Paloma Cariñanos, Cynnamon Dobbs, Silvija Krajter Ostoić, Ana Marija Marin, et al. 2020. “Effects of the COVID-19 Pandemic on the Use and Perceptions of Urban Green Space: An International Exploratory Study.” Urban Forestry & Urban Greening 56: 126888.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling Time</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Social Science",
    "section": "",
    "text": "Welcome\nThis is the website for “Computational Social Science”. This is a course designed by Dr Carmen Cabrera, Prof Francisco Rowe and Dr Elisabetta Pietrostefani, and currently delivered by Dr Carmen Cabrera, Prof Francisco Rowe and Dr Olga Gkountouna, from the Geographic Data Science Lab at the University of Liverpool, United Kingdom. Through a series of hands-on computational notebooks, you will learn data science methods to address social science questions, including supervised and unsupervised machine learning approaches, network analysis and time series analysis methods.\nThe course material is based on the previously taught course on Population Data Science, designed by Prof Francisco Rowe, Dr Carmen Cabrera and Dr Elisabetta Pietrostefani. The previous version of this course is available here.\nThis website is free to use and is licensed under the Attribution-NonCommercial-NoDerivatives 4.0 International. A compilation of this web course is hosted as a GitHub repository that you can access:",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#contacts-for-2025-26",
    "href": "index.html#contacts-for-2025-26",
    "title": "Computational Social Science",
    "section": "Contacts for 2025-26",
    "text": "Contacts for 2025-26\n\nModule lead: Dr Carmen Cabrera - c.cabrera [at] liverpool.ac.uk - Lecturer in Geographic Data Science\nProf Francisco Rowe - fcorowe [at] liverpool.ac.uk - Professor in Population Data Science\nDr Olga Gkountouna - olga.gkountouna [at] liverpool.ac.uk - Lecturer in Geographic Data Science\nFind us in Roxby Building, University of Liverpool, UK",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Overview",
    "section": "",
    "text": "1.1 Aims\nThis module aims to:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "intro.html#aims",
    "href": "intro.html#aims",
    "title": "1  Overview",
    "section": "",
    "text": "provide an introduction to fundamental theories of population data science;\nintroduce students to novel data and approaches to understanding population dynamics and societal change; and,\nequip students with skills and experience to address social science questions using computational approaches.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "intro.html#learning-outcomes",
    "href": "intro.html#learning-outcomes",
    "title": "1  Overview",
    "section": "1.2 Learning Outcomes",
    "text": "1.2 Learning Outcomes\nBy the end of the module, students should be able to:\n\ngain an appreciation of relevant social science concepts to help interpret patterns of population change;\ndevelop an understanding of the types of computational social science methods that are essential for interpreting and analysing new forms of data in the context of population dynamics;\ndevelop the ability to apply different methods to understand population dynamics and societal change;\ngain an appreciation of how computational social science approaches can produce relevant evidence to inform policy debates;\ndevelop critical awareness of modern social science analysis and ethical considerations in the use of new forms of data such as digital traces.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "intro.html#feedback",
    "href": "intro.html#feedback",
    "title": "1  Overview",
    "section": "1.3 Feedback",
    "text": "1.3 Feedback\nFormal assessment of two computational essays. Written assignment-specific feedback will be provided within three working weeks of the submission deadline. Comments will offer an understanding of the mark awarded and identify areas which can be considered for improvement in future assignments.\nVerbal face-to-face feedback. Immediate face-to-face feedback will be provided during computer, discussion and clinic sessions in interaction with staff. This will take place in all live sessions during the semester.\nOnline forum. Asynchronous written feedback will be provided via an online forum. Students are encouraged to contribute by asking and answering questions relating to the module content. Staff will monitor the forum Monday to Friday 9am-5pm, but it will be open to students to make contributions at all times. Response time will vary depending on the complexity of the question and staff availability.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "intro.html#computational-environment",
    "href": "intro.html#computational-environment",
    "title": "1  Overview",
    "section": "1.4 Computational Environment",
    "text": "1.4 Computational Environment\nTo reproduce the code in the book, you need the following software packages:\n\nR-4.3.2\nRStudio 2023.12.0-369\nQuarto 1.4.543\nthe list of libraries in the next section\n\nTo check your version of:\n\nR and libraries run sessionInfo()\nRStudio click help on the menu bar and then About\nQuarto check the version file in the quarto folder on your computer.\n\nTo install and update:\n\nR, download the appropriate version from The Comprehensive R Archive Network (CRAN)\nRStudio, download the appropriate version from Posit\nQuarto, download the appropriate version from the Quarto website\n\n\n1.4.1 List of libraries\nThe list of libraries used in this book is provided below:\n\n“tidyverse”,\n“viridis”\n“viridisLite”\n“ggthemes”\n“patchwork”\n“showtext”\n“RColorBrewer”\n“lubridate”\n“tmap”\n“sjPlot”\n“sf”\n“sp”\n“kableExtra”\n“ggcorrplot”\n“plotrix”\n“cluster”\n“factoextra”\n“igraph”\n“stringr”\n“rpart”\n“rpart.plot”\n“ggplot2”\n“Metrics”\n“caret”\n“randomForest”\n“ranger”\n“wpgpDownloadR”\n“devtools”\n“ggseqplot”\n“tidytext”\n“tm”\n“textdata”\n“topicmodels”\n“RedditExtractoR”\n“stm”\n“dygraphs”\n“plotly”\n“ggpmisc”\n“ggformula”\n“ggimage”\n“modelsummary”\n“gtools”\n“webshot”\n“gridExtra”\n“broom”\n“rtweet”\n“dplyr”\n“ggraph”\n“tidygraph”\n“ggspatial”\n\nYou need to ensure you have installed the list of libraries used in this book, running the following code:\n\n# package names\npackages &lt;- c( \"tidyverse\", \"viridis\", \"viridisLite\", \"ggthemes\", \"patchwork\", \"showtext\", \"RColorBrewer\", \"lubridate\", \"tmap\", \"sjPlot\", \"sf\", \"sp\", \"kableExtra\", \"ggcorrplot\", \"plotrix\", \"cluster\", \"factoextra\", \"igraph\", \"stringr\", \"rpart\", \"rpart.plot\", \"ggplot2\", \"Metrics\", \"caret\", \"randomForest\", \"ranger\", \"devtools\", \"vader\", \"wpgpDownloadR\", \"ggseqplot\", \"tidytext\", \"tm\", \"textdata\", \"topicmodels\", \"RedditExtractoR\", \"stm\", \"dygraphs\", \"plotly\", \"ggpmisc\", \"ggformula\", \"ggimage\", \"modelsummary\", \"gtools\", \"gridExtra\", \"broom\", \"rtweet\", \"webshot\", \"ggraph\", \"tidygraph\", \"ggspatial\")\n\n# install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# packages loading\ninvisible(lapply(packages, library, character.only = TRUE))",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "intro.html#assessment",
    "href": "intro.html#assessment",
    "title": "1  Overview",
    "section": "1.5 Assessment",
    "text": "1.5 Assessment\nThe final module mark is composed of the two computational essays. Together they are designed to cover the materials introduced in the entirety of content covered during the semester. A computational essay is an essay whose narrative is supported by code and computational results that are included in the essay itself. Each teaching week, you will be required to address a set of questions relating to the module content covered in that week, and to use the material that you will produce for this purpose to build your computational essay.\nAssignment 1 (50%) assesses teaching content from Weeks 1 to 5. You are required to use your responses to build your computational essay. Each chapter provides more specific guidance of the tasks and discussion that you are required to consider in your assignment.\nAssignment 2 (50%) assesses teaching content from Weeks 6 to 12. You are required to use your responses to build your computational essay. Each chapter provides more specific guidance of the tasks and discussion that you are required to consider in your assignment.\n\n1.5.1 Format Requirements\nBoth assignments will have the same requirements:\n\nMaximum word count: 2,000 words, excluding figures and references.\nUp to three maps, plot or figures (a figure may include more than one map and/or plot and will only count as one but needs to be integrated in the figure)\nUp to two tables.\n\nAssignments need to be prepared in “Quarto Document” format (i.e. qmd extension) and then converted into a self-contained HTML file that will then be submitted via Turnitin. It is very important that the quarto document is self-contained so that it renders well once submitted. The document should only display content that will be assessed. Intermediate steps do not need to be displayed. Messages resulting from loading packages, attaching data frames, or similar messages do not need to be included as output code. Useful resources to customise your R notebook can be found on Quarto’s website.\nTwo Quarto Document templates will be available via the module Canvas site. You can download these templates as use them for your assignments. Highly recommnded!\nSubmission is electronic only via Turnitin on Canvas.\n\n1.5.1.1 Marking criteria\nThe Standard Environmental Sciences School marking criteria apply, with a stronger emphasis on evidencing the use of regression models, critical analysis of results and presentation standards. In addition to these general criteria, the code and outputs (i.e. tables, maps and plots) contained within the notebook submitted for assessment will be assessed according to the extent of documentation and evidence of expertise in changing and extending the code options illustrated in each chapter. Specifically, the following criteria will be applied:\n\n0-15: no documentation and use of default options.\n16-39: little documentation and use of default options.\n40-49: some documentation, and use of default options.\n50-59: extensive documentation, and edit of some of the options provided in the notebook (e.g. change north arrow location).\n60-69: extensive well organised and easy to read documentation, and evidence of understanding of options provided in the code (e.g. tweaking existing options).\n70-79: all above, plus clear evidence of code design skills (e.g. customising graphics, combining plots (or tables) into a single output, adding clear axis labels and variable names on graphic outputs, etc.).\n80-100: all as above, plus code containing novel contributions that extend/improve the functionality the code was provided with (e.g. comparative model assessments, novel methods to perform the task, etc.).\n\n\n\n\n\nRowe, Francisco. 2021. “Big Data and Human Geography.” http://dx.doi.org/10.31235/osf.io/phz3e.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "1  Overview",
    "section": "",
    "text": "1.1 Aims\nThis module aims to:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "overview.html#aims",
    "href": "overview.html#aims",
    "title": "1  Overview",
    "section": "",
    "text": "provide an introduction to fundamental concepts and questions in computational social science;\nprovide students with hands-on experience in applying data science and social science methods, to analyse novel and large-scale social data; and,\nequip students with the ability to critically evaluate data-driven analyses of societal change in relation to policy-relevant debates, ethical considerations and broader societal implications.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "overview.html#learning-outcomes",
    "href": "overview.html#learning-outcomes",
    "title": "1  Overview",
    "section": "1.2 Learning Outcomes",
    "text": "1.2 Learning Outcomes\nBy the end of the module, students should be able to: - understand key social science concepts and questions to interpret patterns of population dynamics and societal change; - apply data science and social science methods to analyse novel and large-scale social data; - critically evaluate how data-driven analyses of societal change inform policy-relevant debates, demonstrating awareness of ethical, legal and societal issues associated with big and novel forms of data.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "overview.html#feedback",
    "href": "overview.html#feedback",
    "title": "1  Overview",
    "section": "1.3 Feedback",
    "text": "1.3 Feedback\nFormal assessment of two computational essays. Written assignment-specific feedback will be provided within four working weeks of the submission deadline. Comments will offer an understanding of the mark awarded and identify areas which can be considered for improvement in future assignments.\nVerbal face-to-face feedback. Immediate face-to-face feedback will be provided during on-campus sessions in interaction with staff. This will take place in all live sessions during the semester.\nOnline forum. Asynchronous written feedback will be provided via an online forum maintained by the module lead on Microsoft Teams. Students are encouraged to contribute by asking and answering questions relating to the module content. Staff will monitor the forum Monday to Friday 9am-5pm, but it will be open to students to make contributions at all times.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "overview.html#computational-environment",
    "href": "overview.html#computational-environment",
    "title": "1  Overview",
    "section": "1.4 Computational Environment",
    "text": "1.4 Computational Environment\nTo reproduce the code in the book, you need the following software packages:\n\nR-4.5.2\nRStudio 2026.01.0-392\nQuarto 1.8.27\nthe list of libraries in the next section\n\nTo check your version of:\n\nR and libraries run sessionInfo()\nRStudio click help on the menu bar and then About\nQuarto check the version file in the quarto folder on your computer.\n\nTo install and update:\n\nR, download the appropriate version from The Comprehensive R Archive Network (CRAN)\nRStudio, download the appropriate version from Posit\nQuarto, download the appropriate version from the Quarto website\n\n\n1.4.1 List of libraries\nThe list of libraries used in this book is provided below:\n\n“tidyverse”,\n“viridis”\n“viridisLite”\n“ggthemes”\n“patchwork”\n“showtext”\n“RColorBrewer”\n“lubridate”\n“tmap”\n“sjPlot”\n“sf”\n“sp”\n“kableExtra”\n“ggcorrplot”\n“plotrix”\n“cluster”\n“factoextra”\n“igraph”\n“stringr”\n“rpart”\n“rpart.plot”\n“ggplot2”\n“Metrics”\n“caret”\n“randomForest”\n“ranger”\n“wpgpDownloadR”\n“devtools”\n“ggseqplot”\n“tidytext”\n“tm”\n“textdata”\n“topicmodels”\n“RedditExtractoR”\n“stm”\n“dygraphs”\n“plotly”\n“ggpmisc”\n“ggformula”\n“ggimage”\n“modelsummary”\n“gtools”\n“webshot”\n“gridExtra”\n“broom”\n“rtweet”\n“dplyr”\n“ggraph”\n“tidygraph”\n“ggspatial”\n\nYou need to ensure you have installed the list of libraries used in this book, running the following code:\n\n# package names\npackages &lt;- c( \"tidyverse\", \"viridis\", \"viridisLite\", \"ggthemes\", \"patchwork\", \"showtext\", \"RColorBrewer\", \"lubridate\", \"tmap\", \"sjPlot\", \"sf\", \"sp\", \"kableExtra\", \"ggcorrplot\", \"plotrix\", \"cluster\", \"factoextra\", \"igraph\", \"stringr\", \"rpart\", \"rpart.plot\", \"ggplot2\", \"Metrics\", \"caret\", \"randomForest\", \"ranger\", \"devtools\", \"vader\", \"wpgpDownloadR\", \"ggseqplot\", \"tidytext\", \"tm\", \"textdata\", \"topicmodels\", \"RedditExtractoR\", \"stm\", \"dygraphs\", \"plotly\", \"ggpmisc\", \"ggformula\", \"ggimage\", \"modelsummary\", \"gtools\", \"gridExtra\", \"broom\", \"rtweet\", \"webshot\", \"ggraph\", \"tidygraph\", \"ggspatial\")\n\n# install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# packages loading\ninvisible(lapply(packages, library, character.only = TRUE))",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "overview.html#assessment",
    "href": "overview.html#assessment",
    "title": "1  Overview",
    "section": "1.5 Assessment",
    "text": "1.5 Assessment\nThe final module mark is an average of the mark for two computational essays. These assignments are designed to cover the materials introduced during the semester. A computational essay is an essay whose narrative is supported by code and computational results that are included in the essay itself. It seeks to assess your ability to implement code, use the methods taught during class, analyse and critically interpret results, and effectively communicate complex analyses. Each teaching week, you will be required to address a set of questions relating to the module content covered in that week. You will be required to use the material produced to address each question to build your computational essay.\nAssignment 1 (50%) assesses teaching content from Weeks 1 to 5. You are required to use your responses to build your computational essay. Each chapter provides more specific guidance of the tasks and discussion that you are required to consider in your assignment.\nAssignment 2 (50%) assesses teaching content from Weeks 7 to 11. You are required to use your responses to build your computational essay. Each chapter provides more specific guidance of the tasks and discussion that you are required to consider in your assignment.\n\n1.5.1 Format Requirements\nBoth assignments will have the same requirements:\n\nMaximum word count: approximately 2,000 words, excluding figures and references. As per School Assessment Guidelines, over-length submission will be capped at 40%.\nUp to four maps, plot or figures.\nUp to two tables.\n\nAssignments need to be prepared in “Quarto Document” format (i.e. qmd extension) and then converted into a self-contained HTML file that will then be submitted via Turnitin. It is very important that the quarto document is self-contained so that it renders well once submitted. The document should only display content that will be assessed. Intermediate steps do not need to be displayed. Messages resulting from loading packages, attaching data frames, or similar messages do not need to be included as output code. Useful resources to customise your R notebook can be found on Quarto’s website.\nTwo Quarto Document templates will be available via the module Canvas site. You can download these templates as use them for your assignments. Highly recommnded!\nSubmission is electronic only via Turnitin on Canvas.\n\n1.5.1.1 Marking criteria\nThe General School of Environmental Sciences marking rubric (2025-26) applies, with a stronger emphasis on evidencing the use of regression models, critical analysis of results and presentation standards. In addition to these general criteria, the code and outputs (i.e. tables, maps and plots) contained within the notebook submitted for assessment will be assessed according to the extent of documentation and evidence of expertise in changing and extending the code options illustrated in each chapter. Specifically, the following criteria will be applied:\n\n0-15: no documentation and use of default options.\n16-39: little documentation and use of default options.\n40-49: some documentation, and use of default options.\n50-59: extensive documentation, and edit of some of the options provided in the notebook (e.g. change north arrow location).\n60-69: extensive well organised and easy to read documentation, and evidence of understanding of options provided in the code (e.g. tweaking existing options).\n70-79: all above, plus clear evidence of code design skills (e.g. customising graphics, combining plots (or tables) into a single output, adding clear axis labels and variable names on graphic outputs, etc.).\n80-100: all as above, plus code containing novel contributions that extend/improve the functionality the code was provided with (e.g. comparative model assessments, novel methods to perform the task, etc.).\n\n\n\n\n\nCabrera, Carmen, and Francisco Rowe. 2025. “A Systematic Machine Learning Approach to Measure and Assess Biases in Mobile Phone Population Data.” https://doi.org/10.48550/ARXIV.2509.02603.\n\n\nRowe, Francisco. 2021. “Big Data and Human Geography.” http://dx.doi.org/10.31235/osf.io/phz3e.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "intro-css.html",
    "href": "intro-css.html",
    "title": "2  Introducing Computational Social Science",
    "section": "",
    "text": "2.1 Introduction\nComputational social science sits at the intersection between the social sciences and data science. It seeks to develop quantitative and computational approaches to understand human behaviour, social interactions and societal change at scale. Drawing on disciplines such as demography, geography, sociology, economics and political science, computational social science studies how individual actions aggregate into collective patterns, and how social, economic, political and environmental contexts shape social outcomes.\nWhat makes computational social science different from social science, however, is its explicit engagement with the ongoing digital and computational revolution characterised by advances in computing power, data storage, digital connectivity and algorithmic methods (Hilbert and López 2011). While the study of social behaviour and population processes has traditionally relied mostly on structured data sources such as censuses, surveys and administrative records, computational social science increasingly leverages novel data sources.\nParticularly, the digital revolution that started in the 1990s has resulted in a data revolution. Technological advances in computational power, storage and digital network platforms have enabled the production, processing, analysis and storage of large volumes of heterogeneous data. Analysing 1986-2007 data, (Hilbert and López 2011) estimated that the world had already passed the point at which more data were being collected than could be physically stored. They estimated that the global general-purpose computing capacity grew at an annual rate of 58% between 1986 and 2007, exceeding that of global storage capacity (23%). We can now digitally capture and generate forms of data that previously could not easily be recorded, stored or analysed.\nThe unprecedented amount of information that we can now capture through digital technology offers unique opportunities to advance our understanding of micro social behaviour (e.g. individual-level decision making, preferences, interactions) and macro population processes (e.g. structural population processes and trends). New and traditional data sources can be combined to study human behaviour at unprecedented spatial and temporal resolution, often in near real time and at large scale.\nWe can capture and study micro individual behaviours such as time use, purchasing behaviour, communication patterns and mobility trajectories from a variaty of data sources, including mobile phones, social media platforma, smart infrastructure and administrative systems. These behaviours can be aggregated to shed light into macro-level structural processes and trends, such as urban dynamics, labour markets, consumer demand, transport usage, population ageing and political participation. Fundamentally, computational approaches thus have the potential to become a key pillar informing and supporting decision making. They can inform business, public services and governments in addressing major societal issues, such as pandemics, climate change, inequality and migration, influencing influencing policy, practice and governance structures.\nYet, the growing use of large-scale and computational data also poses major conceptual, methodological and ethical challenges (Rowe 2021). These challenges motivate this module. Many contemporary data sources and algorithms are not collected or designed for research purposes, and turning these into rigorous social science knowledge requires a unique combination of computational expertise and domain-specific understanding. Traditionally, university programmes have tended to separate technical training from substantive social science education. This module aims to fill this gap by offering an integrated training in computational methods and social science applications, encouraging students to critically analyse data, design computational studies and interpret results in their social context.\nThe name of this module, Computational Social Science, reflects the inclusive and interdisciplinary perspective we hope to capture. The data and computational revolutions have led to the emergence of a range of sub-disciplines, such as digital demography (Kashyap et al. 2022), computational sociology, network science (Lazer et al. 2009) and geographic data science (Singleton and Arribas-Bel 2019). Computational social science seeks to integrate these perspectives and provide a fertile framework for critique, collaboration and co-creation across these emerging areas of scholarship in the study of human behaviour and social systems.\nSpecifically, this chapter aims to discuss key opportunities and challenges with the use of large-scale digital data and algorithms to analyse social and population dynamics. We place a particular focus on the challenges relating to privacy, bias and privacy issues.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introducing Computational Social Science</span>"
    ]
  },
  {
    "objectID": "intro-css.html#defining-digital-footprint-data",
    "href": "intro-css.html#defining-digital-footprint-data",
    "title": "2  Introducing Population Data Science",
    "section": "2.2 Defining digital footprint data",
    "text": "2.2 Defining digital footprint data\nWe define digital footprint data as:\n\nthe data recorded by digital technology resulting from the interactions of people among themselves or with their social and physical environment, and they can take the form of images, video, text and numbers.\n\nData footprint data are distinctive features in their volume, velocity, variety, exhaustiveness, resolution, relational nature and flexibility (Kitchin 2014). They can take different forms. Traditional data used to be mostly numeric. Digital footprint data has facilitated the collection, storage and analysis of text (e.g. Twitter posts), image (e.g. Instagram photos) and video (e.g. CCTV footage) data.\nMultiple digital systems contribute to the storage and generation of digital footprint data. Kitchin (2014) identified three broad systems directed, automated and volunteered systems. Directed systems comprise digital administrative systems operated by a human recording data on places or people e.g. immigration control, biometric scanning and health records. Automated systems involve digital systems which automatically and autonomously record and process data with little human intervention e.g. mobile phone applications, electronic smartcard ticketing, energy smart meter and traffic sensors. Volunteered systems involve digital spaces in which humans contribute data through interactions on social media platforms (e.g. Twitter and Facebook) or crowdsourcing (e.g. OpenStreetMap and Wikipedia).\n\n\n\nDigital footprint systems",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introducing Computational Social Science</span>"
    ]
  },
  {
    "objectID": "intro-css.html#opportunities-of-digital-footprint-data",
    "href": "intro-css.html#opportunities-of-digital-footprint-data",
    "title": "2  Introducing Population Data Science",
    "section": "2.3 Opportunities of digital footprint data",
    "text": "2.3 Opportunities of digital footprint data\nDigital footprint data offer unique opportunities for the analysis of human population patterns. As Rowe (2021) argues, digital footprint data offer three key promises in relation to traditional data sources, such as surveys and censuses. They generally provide greater spatio-temporal granularity, wider coverage and timeliness.\nDigital footprint data offer high geographic and temporal granularity. Most digital footprint data are time-stamped and geographically referenced with high precision. Digital technology, such as mobile phone and geographical positioning systems enables the generation of a continuous steams of time-stamped location data. Such information thus provides an opportunity to trace and enhance our understanding human populations over highly granular spatial scales and time intervals, going beyond the static representation afforded by most traditional data sources. Spatial human interactions, and how people use and are influenced by their environment, can be analysed in a temporally dynamic way.\nDigital footprint data provide extensive coverage. Contrasting to traditional random sampling, digital footprint data promise information on universal or near-universal population or geographical systems. Social media platforms, such as Twitter generate data to capture the entire universe of Twitter users. Satellite technology produces imagery snapshots to composite a representation of the Earth. Electronic smartcard ticketing systems produce information to capture the population of users in the system. Because the information is typically consistently collected and storage, the coverage of digital footprint data offer the potential to study human behaviour of entire systems at a global scale based on harmonised definitions, which is rarely possible using traditional data sources.\nDigital footprint data are generated in real-time. Unlike traditional systems of data collection and release, digital footprint data can be streamed continuously in real- or near real-time. Commercial transactions are generally recorded on bank ledgers as bank card payments occur at retail shops. Individual mobile phone’s location are captured as applications ping cellular antennas. Such information offer an opportunity to monitor and response to rapidly evolving situations, such as the COVID-19 pandemic (Green, Pollock, and Rowe 2021), natural disasters (Rowe 2022) and conflicts (Rowe, Neville, and González-Leonardo 2022).\nWe also loudly and clearly argue that while digital footprint data should be seen as a key asset to support government and business decision making processes, they should not be considered at the expenses of traditional data sources. Digital footprint data and traditional data sources should be used to complement each another. As indicated earlier, digital footprint data are the by-product of administrative processes or services. They were not designed with the aim of doing research. They require considerable work of data re-engineering to re-purpose them and turn them into an analysis-ready data product that can be used for further analysis (Arribas-Bel et al. 2021). Yet, as we will discuss below significant challenges remain. As the saying goes “all data are dirty, but some data are useful”. This quote used in the data science community to convey the idea that data are often imperfect, but they can still be used to gain valuable insights. Our message is that digital footprint data and traditional data sources should be triangulated to leverage on their strengths and mitigate their weaknesses.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introducing Computational Social Science</span>"
    ]
  },
  {
    "objectID": "intro-css.html#challenges-of-digital-footprint-data",
    "href": "intro-css.html#challenges-of-digital-footprint-data",
    "title": "2  Introducing Population Data Science",
    "section": "2.4 Challenges of digital footprint data",
    "text": "2.4 Challenges of digital footprint data\nDigital footprint data also impose key conceptual, methodological and ethical challenges. In this section, we provide a brief explanation of challenges in these areas, focusing particularly on issues around biases, privacy, ethics and new methods. We focus on these issues because they are of practical importance and probably of most interest to the readers of this book. Excellent discussions have been written and, if you are interested in learning more about the challenges relating to digital footprint data, we recommend Kitchin (2014), Cesare et al. (2018), Lazer et al. (2020) and Rowe (2021).\n\n2.4.1 Conceptual challenges\nConceptually, the emergence of digital footprint data has led to the rethinking and questioning of existing theoretical social science approaches (Franklin 2022). On the one hand, digital footprint data provide an opportunity to explore existing theories or hypotheses through different lens and test the consistency of existing beliefs. For example, economics theories discuss the existence of temporal and spatial equilibrium. Resulting hypotheses are generally tested through mathematical theoretical models or empirical analyses relying on temporally static data. The existence of equilibrium has thus remained hard to assess. Digital footprint data provide an opportunity to empirically test temporal and spatial equilibrium ideas based on suitable temporally dynamic data. They can enable the testing of cause and impact hypotheses, rather than only focusing on static associations.\nOn the other hand, digital footprint data sparked new questions. Digital footprint data provide data on previously unmeasured activities. Data now capture activities that were previously difficult to quantify, such as personal communications, social networks, search and information gathering, and location data. These data offer an opportunity to develop new questions expanding existing theories by looking inside the “black box” of households, organisations and markets. They may also open the door to developing entirely new questions such as the role of digital technology in shaping human behaviour, and the role of artificial intelligence on productivity and financial markets.\n\n\n2.4.2 Methodological challenges\nMethodologically, the need for a wide and new set of digital skills and expertise to handle, store and analyse large volumes of data is a key challenge. As indicated earlier, digital footprint data are not created for research purposes. They need to be reengineered for research. Large streams of digital footprint data cannot be stored on local memory. They can rarely be read as a single unit on a local computer and may involve performing the same task numerous times in regular basis, requiring therefore large storage, computational capacity and computer science expertise. The manipulation and storage of digital footprint data often require technical expertise in data management systems, such as SQL, Google Cloud Storage and Amazon S3, as well as in efficient computing involving expertise in distributed computing systems and parallelisation frameworks. The analysis and modelling of digital footprint data may entail competencies in the application of machine learning and artificial intelligence. While these competencies generally form part of a computer science programme, they are rarely taught in an integrated framework focusing on addressing societal or business challenges relating to human populations - where the key focus is their application.\nAn additional methodological challenge is the presence of biases in digital footprint data. Digital footprint data are representative of a specific segment of the population but little is known which segments and how their representation varies across data sets and digital technology. Digital footprint data may comprise multiple sources of biases. They may reflect differences in the use of a digital device (e.g. mobile phone) and/or a piece of digital technology (e.g. a mobile phone application) Schlosser et al. (2021) . They may also reflect differences in frequency in the use of digital technology (e.g. number of times an individual uses a mobile phone application) - and this frequency may in turn reflect differences in algorithmic decisions embedded in digital platforms, such as suggesting content based on prior interactions to increase engagement with a given mobile phone application. Some work has been done on assessing biases as well as developing approaches to mitigate their influence Ribeiro, Benevenuto, and Zagheni (2020).\n\n\n2.4.3 Ethical challenges\nPrivacy represents a major ethical challenge. Digital footprint data are highly sensitive, and hence, anonymisation and disclosure control are required. Individual records must be anonymised so they are not identifiable. The high degree of granularity and personal information of these records may and have been used in ethically questionable ways; for example, Cambridge Analytica used information of Facebook users to segment the population and target politically motivated content (Cadwalladr and Graham-Harrison 2018). Anonymising information, however, imposes a key challenge as there is a trade-off between accuracy and privacy (Petti and Flaxman 2020). Anonymisation may reduce the usability of data. The greater the degree of privacy, the lower is the degree of accuracy of the resulting data and vice versa. Identifying the optimal point balancing the privacy-accuracy trade-off is the key challenge. If doing incorrectly, we could end up drawing inferences that do not reflect the actual population processes displayed in the data, or have artificially been encoded in the data through noise or reshuffling. The application of data differential privacy to the US census provides a recent good example of this challenge. An emblematic case is New York’s Liberty Island which has no resident population, but official US census reported 48 residents which was the result of adding statistical noise to the data, in order to enhance privacy.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introducing Computational Social Science</span>"
    ]
  },
  {
    "objectID": "intro-css.html#conclusion",
    "href": "intro-css.html#conclusion",
    "title": "2  Introducing Computational Social Science",
    "section": "2.5 Conclusion",
    "text": "2.5 Conclusion\nLarge-scale digital trace data and computational algorithms present unique oppotunites to enhance our understanding of human behaviours, social interactions and population processes, and to support individual, business and government decision-making. Businesses have used data-driven methods to segment their consumer populations and improve the targeting of marketing content, products and services, ultimately increasing sales and revenue (Dolega, Rowe, and Branagan 2021). Governments and health care institutions, particularly during the COVID-19 pandemic, have leverage digital traces to monitor the spread of disease and develop appropriate mitigation responses (Green, Pollock, and Rowe 2021).\nHowever, the use of contemporary data and algorithms poses major conceptual, methodological and ethical challenges that must be addressed to unleash their full potential. These challenges include biases in data and models, limits to inference and interpretability, risks to privacy and the responsible governance of algorithmic systems. The aim of this book is to address of the key methodological challenges in Computational Social Sciences. In particular, the book provides applied training in the practical use of statistical, machine learning and AI approaches to analyse large-scale data and to advance our understanding of human behaviour and social and population processes.\n\n\n\n\nArribas-Bel, Dani, Mark Green, Francisco Rowe, and Alex Singleton. 2021. “Open Data Products-A Framework for Creating Valuable Analysis Ready Data.” Journal of Geographical Systems 23 (4): 497–514. https://doi.org/10.1007/s10109-021-00363-5.\n\n\nCabrera, Carmen, and Ruth Neville. 2025. “Widely Used but Barely Trusted: Understanding Student Perceptions on the Use of Generative AI in Higher Education.” Perspectives: Policy and Practice in Higher Education, December, 1–14. https://doi.org/10.1080/13603108.2025.2595453.\n\n\nCabrera, Carmen, and Francisco Rowe. 2025. “A Systematic Machine Learning Approach to Measure and Assess Biases in Mobile Phone Population Data.” https://doi.org/10.48550/ARXIV.2509.02603.\n\n\nCabrera-Arnau, Carmen, Chen Zhong, Michael Batty, Ricardo Silva, and Soong Moon Kang. 2022. “Inferring Urban Polycentricity from the Variability in Human Mobility Patterns.” arXiv. https://doi.org/10.48550/ARXIV.2212.03973.\n\n\nCadwalladr, Carole, and Emma Graham-Harrison. 2018. “Revealed: 50 Million Facebook Profiles Harvested for Cambridge Analytica in Major Data Breach.” The Guardian 17 (1): 22.\n\n\nCesare, Nina, Hedwig Lee, Tyler McCormick, Emma Spiro, and Emilio Zagheni. 2018. “Promises and Pitfalls of Using Digital Traces for Demographic Research.” Demography 55 (5): 1979–99. https://doi.org/10.1007/s13524-018-0715-2.\n\n\nDolega, Les, Francisco Rowe, and Emma Branagan. 2021. “Going Digital? The Impact of Social Media Marketing on Retail Website Traffic, Orders and Sales.” Journal of Retailing and Consumer Services 60 (May): 102501. https://doi.org/10.1016/j.jretconser.2021.102501.\n\n\nFranklin, Rachel. 2022. “Quantitative Methods II: Big Theory.” Progress in Human Geography 47 (1): 178–86. https://doi.org/10.1177/03091325221137334.\n\n\nGreen, Mark, Frances Darlington Pollock, and Francisco Rowe. 2021. “New Forms of Data and New Forms of Opportunities to Monitor and Tackle a Pandemic.” In, 423–29. Springer International Publishing. https://doi.org/10.1007/978-3-030-70179-6_56.\n\n\nHilbert, Martin, and Priscila López. 2011. “The World’s Technological Capacity to Store, Communicate, and Compute Information.” Science 332 (6025): 60–65. https://doi.org/10.1126/science.1200970.\n\n\nKashyap, Ridhi, R. Gordon Rinderknecht, Aliakbar Akbaritabar, Diego Alburez-Gutierrez, Sofia Gil-Clavel, André Grow, Jisu Kim, et al. 2022. “Digital and Computational Demography.” http://dx.doi.org/10.31235/osf.io/7bvpt.\n\n\nKitchin, Rob. 2014. “Big Data, New Epistemologies and Paradigm Shifts.” Big Data & Society 1 (1): 205395171452848. https://doi.org/10.1177/2053951714528481.\n\n\nLazer, David, Alex Pentland, Lada Adamic, Sinan Aral, Albert-László Barabási, Devon Brewer, Nicholas Christakis, et al. 2009. “Computational Social Science.” Science 323 (5915): 721–23. https://doi.org/10.1126/science.1167742.\n\n\nLazer, David, Alex Pentland, Duncan J. Watts, Sinan Aral, Susan Athey, Noshir Contractor, Deen Freelon, et al. 2020. “Computational Social Science: Obstacles and Opportunities.” Science 369 (6507): 1060–62. https://doi.org/10.1126/science.aaz8170.\n\n\nLiang, Hai, and King-wa Fu. 2015. “Testing Propositions Derived from Twitter Studies: Generalization and Replication in Computational Social Science.” Edited by Zi-Ke Zhang. PLOS ONE 10 (8): e0134270. https://doi.org/10.1371/journal.pone.0134270.\n\n\nPetti, Samantha, and Abraham Flaxman. 2020. “Differential Privacy in the 2020 US Census: What Will It Do? Quantifying the Accuracy/Privacy Tradeoff.” Gates Open Research 3 (April): 1722. https://doi.org/10.12688/gatesopenres.13089.2.\n\n\nRibeiro, Filipe N., Fabrício Benevenuto, and Emilio Zagheni. 2020. “How Biased Is the Population of Facebook Users? Comparing the Demographics of Facebook Users with Census Data to Generate Correction Factors.” 12th ACM Conference on Web Science, July. https://doi.org/10.1145/3394231.3397923.\n\n\nRowe, Francisco. 2021. “Big Data and Human Geography.” http://dx.doi.org/10.31235/osf.io/phz3e.\n\n\n———. 2022. “Using Digital Footprint Data to Monitor Human Mobility and Support Rapid Humanitarian Responses.” Regional Studies, Regional Science 9 (1): 665–68. https://doi.org/10.1080/21681376.2022.2135458.\n\n\nRowe, Francisco, Ruth Neville, and Miguel González-Leonardo. 2022. “Sensing Population Displacement from Ukraine Using Facebook Data: Potential Impacts and Settlement Areas.” http://dx.doi.org/10.31219/osf.io/7n6wm.\n\n\nSchlosser, Frank, Vedran Sekara, Dirk Brockmann, and Manuel Garcia-Herranz. 2021. “Biases in Human Mobility Data Impact Epidemic Modeling.” https://doi.org/10.48550/ARXIV.2112.12521.\n\n\nSingleton, Alex, and Daniel Arribas-Bel. 2019. “Geographic Data Science.” Geographical Analysis 53 (1): 61–75. https://doi.org/10.1111/gean.12194.\n\n\nWang, Yikang, Chen Zhong, Qili Gao, and Carmen Cabrera-Arnau. 2022. “Understanding Internal Migration in the UK Before and During the COVID-19 Pandemic Using Twitter Data.” Urban Informatics 1 (1): 15.\n\n\nZagheni, Emilio, and Ingmar Weber. 2015. “Demographic Research with Non-Representative Internet Data.” Edited by Nikolaos Askitas and Professor Professor Klaus F. Zimmermann. International Journal of Manpower 36 (1): 13–25. https://doi.org/10.1108/ijm-12-2014-0261.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introducing Computational Social Science</span>"
    ]
  },
  {
    "objectID": "intro-css.html#data-and-algorithms-in-computational-social-science",
    "href": "intro-css.html#data-and-algorithms-in-computational-social-science",
    "title": "2  Introducing Computational Social Science",
    "section": "2.2 Data and algorithms in Computational Social Science",
    "text": "2.2 Data and algorithms in Computational Social Science\nContemporary computational social science draws on a wide range of data sources, including traditional structured data such as censuses, surveys and administrative records, as well as newer forms of large-scale digital data often referred to as `digital trace data’. These newer froms are distinctive in their volume, velocity, variety, exhaustiveness, resolution, relational nature and flexibility (Kitchin 2014). Traditionally, social science data was mostly numeric and highly structured. The expansion of digital technologies has facilitated the collection, storage and analysis of unstructured and semi-structured data, including text (e.g. social media posts), images (e.g. photographs and satellite imagery) and video (e.g. CCTV footage). As a result, computational social science increasingly integrates heterogeneous data types within a single analytical framework.\nMultiple digital and administrative systems contribute to the generation and storage of contemporary social data. Kitchin (2014) identified three broad systems: directed, automated and volunteered systems. Directed systems comprise digital administrative systems operated by humans recording data on places or people, e.g. immigration control, biometric scanning and health records. Automated systems involve digital infrastructures that automatically and autonomously record and process data with little human intervention, e.g. mobile phone networks, electronic smartcard ticketing, energy smart meters and traffic sensors. Volunteered systems involve digital spaces in which individuals actively contribute data through interactions on online platforms (e.g. Twitter and Facebook) or through crowdsourcing initiatives (e.g. OpenStreetMap and Wikipedia).\n\nWhile data constitute the empirical foundation of Computational Social Science, its analysis and interpretation depends on the use of statistical, computational and algorithmic methods. Large-scale and high-dimensional data cannot be analysed using only traditional tools. Instead, computational social science draws on a broad range of methods, including statistical modelling, machine learning, network analysis and spatial analysis, to identify patterns, make predictions and test hypotheses. Algorithms play a central role in transforming raw data into knowledge in social science. They are used to clean and preprocess data, detect structure in complex datasets, model relationships between individuals and groups, and simulate social processes. At the same time, algorithms are not “neutral”, as they are often based on assumptions, design choices and training data that may influence the patterns they reveal. Understanding how algorithms operate and how they interact with data, is therefore a core component ability in Computational Social Science.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introducing Computational Social Science</span>"
    ]
  },
  {
    "objectID": "intro-css.html#opportunities-of-data-and-algorithms",
    "href": "intro-css.html#opportunities-of-data-and-algorithms",
    "title": "2  Introducing Computational Social Science",
    "section": "2.3 Opportunities of data and algorithms",
    "text": "2.3 Opportunities of data and algorithms\nLarge-scale digital data and computational algorithms offer unique opportunities for the analysis of human behaviour and population dynamics. As Rowe (2021) argues, contemporary digital trace data offers three key promises in relation to traditional data sources, such as surveys and censuses. They generally provide greater spatio-temporal granularity, wider coverage and timeliness. When combined with advances in statistical and machine learning methods, these data enable new forms of measurement, modelling and inference in the social sciences.\nLarge-scale digital data offer high geographic and temporal granularity. Most digital footprint data are time-stamped and geographically referenced with high precision. Digital technologies such as mobile phone networks and Global Positioning Systems (GPS) enables the generation of a continuous streams of time-stamped location data. Such information thus provides an opportunity to trace and enhance our understanding human populations over highly granular spatial scales and time intervals, going beyond the static representation afforded by most traditional data sources. Spatial interactions, mobility patterns and they ways in which people use and are influenced by their environment can be analysed in a temporally dynamic way.\nLarge-scale digital data also provide extensive coverage. In contrast to traditional random sampling, many contemporary data sources capture universal or near-universal populations or geographical systems. Social media platforms, such as Instagram, generate data to capture the entire universe of Instagram users. Satellite technologies produce imagery snapshots that can be composited to represent the Earth. Electronic smartcard ticketing systems produce information to capture the population of users within transport networks. Because these data sources are typically collected consistently and at scale, they offer the potential to study human behaviour and social systems at regional, national and even global scales based on harmonised definitions, which is rarely possible using traditional data sources.\nA further opportunity lies in the timeliness of contemporary data and algorithms. Unlike traditional systems of data collection and release, many digital data sources can be streamed continuously in real- or near real-time. Commercial transactions are recorded as bank card payments occur at retail outlets. Individual mobile phone locations are captured as applications interact with cellular antennas. When combined with automated algorithms for data processing and analysis, such information offers an opportunity to monitor and respond to rapidly evolving situations, such as the COVID-19 pandemic Wang et al. (2022), natural disasters (Rowe 2022) and conflicts (Rowe, Neville, and González-Leonardo 2022).\nThe opportunities offered by data are inseparable from advances in algorithms and computational methods. Machine learning, artificial intelligence (AI), network analysis and spatial models enable researchers to extract structure from high-dimensional data, detect patterns that are not visible to traditional methods, and build predictive and explanatory models of social processes. Algorithms make it possible to integrate heterogeneous data sources, automate large-scale analyses and simulate complex social systems, extending the scope of questions that can be addressed in the social sciences.\nWe also argue that while large-scale digital data and computational methods should be seen as key assets to support government and business decision-making processes, they should not be considered at the expense of traditional data sources. Contemporary digital trace data and traditional data sources should be used to complement one another. As indicated earlier, many digital data sources by-products of administrative processes or services and were not designed for research purposes. They require considerable work of data re-engineering to be re-purposed into an analysis-ready products (Arribas-Bel et al. 2021). As the saying goes “all data are dirty, but some data are useful”. Our message is that traditional data, contemporary data and computational algorithms should be triangulated to leverage their respectivestrengths and mitigate their weaknesses.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introducing Computational Social Science</span>"
    ]
  },
  {
    "objectID": "intro-css.html#current-challenges-of-computational-social-science",
    "href": "intro-css.html#current-challenges-of-computational-social-science",
    "title": "2  Introducing Computational Social Science",
    "section": "2.4 Current challenges of Computational Social Science",
    "text": "2.4 Current challenges of Computational Social Science\nLarge-scale digital data and computational algorithms also impose key conceptual, methodological and ethical challenges. In this section, we provide a brief discussion of challenges in these areas, focusing particularly on issues relating to bias, privacy, ethics and methods. We focus on these issues because they are of practical importance and probably to be of greatest interest to the readers of this book. Excellent discussions of these challenges can be found in Kitchin (2014), Cesare et al. (2018), Lazer et al. (2020), Rowe (2021) and Cabrera and Rowe (2025).\n\n2.4.1 Conceptual challenges\nConceptually, the emergence of large-scale digital data and computational methods has led to a rethinking and questioning of existing theoretical approaches in the social sciences (Franklin 2022). On the one hand, contemporary data and algorithms provide new opportunities to explore existing theories and hypotheses through different lenses and to test the consistency of long-standing beliefs. For example, economics theories discuss the existence of temporal and spatial equilibrium. Resulting hypotheses are generally tested through mathematical models or empirical analyses relying on temporally static data. The existence of equilibrium has therefore remained diffigult to assess empirically. The availability of temporally dynamic data, combined with computational modelling, provides new opportunities to test temporal and spatial equilibrium using fine-grainged longitudinal information, enabling the analysis of causal processes, rather than only focusing on static associations.\nOn the other hand, contemporary data and algorithms also raise fundamentally new conceptual questions. New data sources capture activities that were previously difficult to measure, such as personal communications, social networks, information seeking and fine-grained mobility patterns (Cabrera-Arnau et al. 2022). These data offer opportunities to expand existing theories by opening the “black box” of households, organisations and markets. They may also motivate entirely new research questions, such as the role of digital technologies in shaping social behaviour, the influence of algorithmic systems on decision making, and the impact of AI on productivity, labour markets and financial systems (Cabrera and Neville 2025).\nAt the same time, the increasing role of algorithms raises questions about explanation and interpretation. Many machine learning models prioritise predictive accuracy over interpretability. This hallenges traditional notions of causal inference and theory testing in the social sciences. Understanding how to reconcile predictive modelling with explanatory aims is a central conceptual challenge for Computational Social Science.\n\n\n2.4.2 Methodological challenges\nMethodologically, the need for a wide and new set of computational skills to handle, store and analyse large volumes of data represents a major challenge. Many contemporary data sources are not created for research purposes and must be re-engineered for scientific analysis. Large data streams cannot be stored on local machines, cannot easily be processed as a single unit, and often require repeated processing over time. This requires storage capacity, computational infrastructure and computer science expertise.\nThe manipulation and storage of large-scale data often require technical expertise in data management systems such as SQL, Google Cloud Storage and Amazon S3, as well as in efficient computing frameworks for distributed and parallel processing. The analysis and modelling of contemporary data increasingly entail competencies in machine learning, AI, network analysis and simulation. While these competencies are commonly taught within computer science programmes, they are rarely integrated with substantive training in social science theory and applied problem solving.\nAn additional methodological challenge concerns biases in both data and algorithms. Many contemporary data sources represent specific segments of the population, but little is often known about which segments and are over or under-represented and how their representation varies across data sets and contexts. Biases reflect differences in the use and access to digital technologies, patterns of use, or differences in the frequency and intensity of interaction with digital systems Cabrera and Rowe (2025). These biases may be further amplified by algorithmic decisions embedded in platforms, such as content recommendation systems designed to maximise engagement.\nMoreover, machine learning models trained on biased data may reproduce or amplify existing social inequalities. Considerable work has been devoted to measuring, understanding and mitigating such biases Ribeiro, Benevenuto, and Zagheni (2020), but developing robust methods for bias correction and uncertainty quantification remains an open methodological challenge.\n\n\n2.4.3 Ethical challenges\nEthical issues represent a central challenge for Computational Social Science. Privacy is perhaps the most prominent concern. Many contemporary data sources contain highly sensitive personal information, and require anonymisaiton and disclosure disclosure control. Individual records must be protected to prevent re-identification. Yet, the high degree of spatial and temporal granularity of these data increases the risk that individuals can be identified, even after anonymisation.\nHigh-provile cases illustrate the ethical risks asociated with the misuse of data and algorithms. For example, Cambridge Analytica used information from Facebook users to segment the population and target politically motivated content (Cadwalladr and Graham-Harrison 2018). And more generally, algorithmic systems are increasingly used to influence information exposure, political participation and consumer behaviour. This raises important concerns about social manipulation, transparency and accountability.\nAnonymising information, however, introduces a trade-off between accuracy and privacy (Petti and Flaxman 2020). The greater the degree of privacy protection, the lower the potential degree of accuracy and utility of the resulting data and vice versa. Identifying an appropriate balance between these objectives is a key ethical and methodological challenge in Computational Social Science. If handled incorrectly, privacy-preserving techniques may distort population-level patterns and introduce artificial structure to the data. The application of data differential privacy to the US census provides a recent good example of this challenge. An emblematic case is New York’s Liberty Island which has no resident population, but official US census reported 48 residents which was the result of adding statistical noise to the data, in order to enhance privacy.\nBeyond privacy, ethical challenges also concern fairness, accountability and governance. Algorithms may systematically disadvantage certain social groups, operate obscurely or be deployed without adequate oversight. Ensuring that data and algorithms are used responsibly, transparently and in the public interest remains a central challenge for Computational Social Science.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introducing Computational Social Science</span>"
    ]
  }
]